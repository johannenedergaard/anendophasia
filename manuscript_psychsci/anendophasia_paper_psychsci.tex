% Options for packages loaded elsewhere
\PassOptionsToPackage{unicode}{hyperref}
\PassOptionsToPackage{hyphens}{url}
%
\documentclass[
  man,a4paper,floatsintext]{apa6}
\usepackage{amsmath,amssymb}
\usepackage{iftex}
\ifPDFTeX
  \usepackage[T1]{fontenc}
  \usepackage[utf8]{inputenc}
  \usepackage{textcomp} % provide euro and other symbols
\else % if luatex or xetex
  \usepackage{unicode-math} % this also loads fontspec
  \defaultfontfeatures{Scale=MatchLowercase}
  \defaultfontfeatures[\rmfamily]{Ligatures=TeX,Scale=1}
\fi
\usepackage{lmodern}
\ifPDFTeX\else
  % xetex/luatex font selection
\fi
% Use upquote if available, for straight quotes in verbatim environments
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
\IfFileExists{microtype.sty}{% use microtype if available
  \usepackage[]{microtype}
  \UseMicrotypeSet[protrusion]{basicmath} % disable protrusion for tt fonts
}{}
\makeatletter
\@ifundefined{KOMAClassName}{% if non-KOMA class
  \IfFileExists{parskip.sty}{%
    \usepackage{parskip}
  }{% else
    \setlength{\parindent}{0pt}
    \setlength{\parskip}{6pt plus 2pt minus 1pt}}
}{% if KOMA class
  \KOMAoptions{parskip=half}}
\makeatother
\usepackage{xcolor}
\usepackage{graphicx}
\makeatletter
\def\maxwidth{\ifdim\Gin@nat@width>\linewidth\linewidth\else\Gin@nat@width\fi}
\def\maxheight{\ifdim\Gin@nat@height>\textheight\textheight\else\Gin@nat@height\fi}
\makeatother
% Scale images if necessary, so that they will not overflow the page
% margins by default, and it is still possible to overwrite the defaults
% using explicit options in \includegraphics[width, height, ...]{}
\setkeys{Gin}{width=\maxwidth,height=\maxheight,keepaspectratio}
% Set default figure placement to htbp
\makeatletter
\def\fps@figure{htbp}
\makeatother
\setlength{\emergencystretch}{3em} % prevent overfull lines
\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}
\setcounter{secnumdepth}{-\maxdimen} % remove section numbering
% Make \paragraph and \subparagraph free-standing
\ifx\paragraph\undefined\else
  \let\oldparagraph\paragraph
  \renewcommand{\paragraph}[1]{\oldparagraph{#1}\mbox{}}
\fi
\ifx\subparagraph\undefined\else
  \let\oldsubparagraph\subparagraph
  \renewcommand{\subparagraph}[1]{\oldsubparagraph{#1}\mbox{}}
\fi
\newlength{\cslhangindent}
\setlength{\cslhangindent}{1.5em}
\newlength{\csllabelwidth}
\setlength{\csllabelwidth}{3em}
\newlength{\cslentryspacingunit} % times entry-spacing
\setlength{\cslentryspacingunit}{\parskip}
\newenvironment{CSLReferences}[2] % #1 hanging-ident, #2 entry spacing
 {% don't indent paragraphs
  \setlength{\parindent}{0pt}
  % turn on hanging indent if param 1 is 1
  \ifodd #1
  \let\oldpar\par
  \def\par{\hangindent=\cslhangindent\oldpar}
  \fi
  % set entry spacing
  \setlength{\parskip}{#2\cslentryspacingunit}
 }%
 {}
\usepackage{calc}
\newcommand{\CSLBlock}[1]{#1\hfill\break}
\newcommand{\CSLLeftMargin}[1]{\parbox[t]{\csllabelwidth}{#1}}
\newcommand{\CSLRightInline}[1]{\parbox[t]{\linewidth - \csllabelwidth}{#1}\break}
\newcommand{\CSLIndent}[1]{\hspace{\cslhangindent}#1}
\ifLuaTeX
\usepackage[bidi=basic]{babel}
\else
\usepackage[bidi=default]{babel}
\fi
\babelprovide[main,import]{english}
% get rid of language-specific shorthands (see #6817):
\let\LanguageShortHands\languageshorthands
\def\languageshorthands#1{}
% Manuscript styling
\usepackage{upgreek}
\captionsetup{font=singlespacing,justification=justified}

% Table formatting
\usepackage{longtable}
\usepackage{lscape}
% \usepackage[counterclockwise]{rotating}   % Landscape page setup for large tables
\usepackage{multirow}		% Table styling
\usepackage{tabularx}		% Control Column width
\usepackage[flushleft]{threeparttable}	% Allows for three part tables with a specified notes section
\usepackage{threeparttablex}            % Lets threeparttable work with longtable

% Create new environments so endfloat can handle them
% \newenvironment{ltable}
%   {\begin{landscape}\centering\begin{threeparttable}}
%   {\end{threeparttable}\end{landscape}}
\newenvironment{lltable}{\begin{landscape}\centering\begin{ThreePartTable}}{\end{ThreePartTable}\end{landscape}}

% Enables adjusting longtable caption width to table width
% Solution found at http://golatex.de/longtable-mit-caption-so-breit-wie-die-tabelle-t15767.html
\makeatletter
\newcommand\LastLTentrywidth{1em}
\newlength\longtablewidth
\setlength{\longtablewidth}{1in}
\newcommand{\getlongtablewidth}{\begingroup \ifcsname LT@\roman{LT@tables}\endcsname \global\longtablewidth=0pt \renewcommand{\LT@entry}[2]{\global\advance\longtablewidth by ##2\relax\gdef\LastLTentrywidth{##2}}\@nameuse{LT@\roman{LT@tables}} \fi \endgroup}

% \setlength{\parindent}{0.5in}
% \setlength{\parskip}{0pt plus 0pt minus 0pt}

% Overwrite redefinition of paragraph and subparagraph by the default LaTeX template
% See https://github.com/crsh/papaja/issues/292
\makeatletter
\renewcommand{\paragraph}{\@startsection{paragraph}{4}{\parindent}%
  {0\baselineskip \@plus 0.2ex \@minus 0.2ex}%
  {-1em}%
  {\normalfont\normalsize\bfseries\itshape\typesectitle}}

\renewcommand{\subparagraph}[1]{\@startsection{subparagraph}{5}{1em}%
  {0\baselineskip \@plus 0.2ex \@minus 0.2ex}%
  {-\z@\relax}%
  {\normalfont\normalsize\itshape\hspace{\parindent}{#1}\textit{\addperi}}{\relax}}
\makeatother

% \usepackage{etoolbox}
\makeatletter
\patchcmd{\HyOrg@maketitle}
  {\section{\normalfont\normalsize\abstractname}}
  {\section*{\normalfont\normalsize\abstractname}}
  {}{\typeout{Failed to patch abstract.}}
\patchcmd{\HyOrg@maketitle}
  {\section{\protect\normalfont{\@title}}}
  {\section*{\protect\normalfont{\@title}}}
  {}{\typeout{Failed to patch title.}}
\makeatother

\usepackage{xpatch}
\makeatletter
\xapptocmd\appendix
  {\xapptocmd\section
    {\addcontentsline{toc}{section}{\appendixname\ifoneappendix\else~\theappendix\fi\\: #1}}
    {}{\InnerPatchFailed}%
  }
{}{\PatchFailed}
\keywords{inner speech, rhyme judgments, categorization, task switching, verbal working memory, individual differences}
\usepackage{lineno}

\linenumbers
\usepackage{csquotes}
\usepackage{tabu}
\usepackage{float}
\usepackage{caption}
\captionsetup[table]{textfont={it}, labelfont={bf}, singlelinecheck=false, labelsep=newline}
\ifLuaTeX
  \usepackage{selnolig}  % disable illegal ligatures
\fi
\IfFileExists{bookmark.sty}{\usepackage{bookmark}}{\usepackage{hyperref}}
\IfFileExists{xurl.sty}{\usepackage{xurl}}{} % add URL line breaks if available
\urlstyle{same}
\hypersetup{
  pdftitle={Not everybody has an inner voice: Behavioral consequences of anendophasia},
  pdflang={en-EN},
  pdfkeywords={inner speech, rhyme judgments, categorization, task switching, verbal working memory, individual differences},
  hidelinks,
  pdfcreator={LaTeX via pandoc}}

\title{Not everybody has an inner voice: Behavioral consequences of anendophasia}
\author{\phantom{0}}
\date{}


\shorttitle{Anendophasia}

\affiliation{\phantom{0}}

\abstract{%
It is commonly assumed that inner speech -- the experience of thought as occurring in a natural language -- is a human universal. Recent evidence, however, suggests that the experience of inner speech in adults varies from near constant to non-existent. We propose a name for a lack of the experience of inner speech -- anendophasia -- and report four studies examining some of its behavioral consequences. We found that people who report low levels of inner speech (N = 46) have lower performance on a verbal working memory task and have more difficulty performing rhyme judgments compared to people who report high levels of inner speech (N = 47). Task switching performance - previously linked to endogenous verbal cueing - and categorical effects on perceptual judgments were unrelated to differences in inner speech.

\textbf{Statement of Relevance}

Most people report experiencing an inner voice, and believe that it plays an important role in their daily lives. However, others report that they do not experience such an inner voice. Although these differences are stable, we do not know whether they have any consequences for how people solve problems and act in the world. In this article, we found that people with less inner speech differed from people with more inner speech on some tasks that we thought would involve inner speech, but not others. It is important to understand such individual differences in inner speech use because it has consequences for how we discuss the role of inner speech generally in human life.
}



\begin{document}
\maketitle

\setcounter{secnumdepth}{5}

\hypertarget{introduction}{%
\section{Introduction}\label{introduction}}

Everyone, it is often said, has an inner voice: `Daily, human beings are engaged in a form of inner dialogue, which enables them to {[}engage in{]} high-level cognition, including self-control, self-attention and self-regulation.': (Chella \& Pipitone, 2020, p. 287); `We all hear a voice inside our brain, commonly called ``inner voice'', ``inner speech'' or referred to as ``verbal thoughts''\,' (Perrone-Bertolotti, Rapin, Lachaux, Baciu, and Loevenbruck (2014), p.~22). Most people do report experiencing inner speech (Alderson-Day \& Fernyhough, 2015; Heavey \& Hurlburt, 2008; Morin, Duhnych, \& Racy, 2018) and because we often assume that our experiences mirror those of others, the majority experience comes to be viewed as universal (Lupyan, Uchiyama, Thompson, \& Casasanto, 2023).
The assumption that everyone has an inner voice has served as a stepping stone for research into the functions of inner speech -- if everyone has it, it must be important. Speculations have ranged from the idea that natural language constitutes (at least some types of) thought (Bermúdez, 2007; Carruthers, 2002; Clark, 1998; Frankish, 2018; Gauker, 2011) or is necessary for self-awareness (Morin, 2018) to investigations of connections between inner speech and specific processes such as cognitive control (Alderson-Day \& Fernyhough, 2015; Cragg \& Nation, 2010; e.g., Emerson \& Miyake, 2003; Morin et al., 2018), behavioral control (e.g., Nedergaard, Christensen, \& Wallentin, 2023), and planning and problem-solving (Lidstone, Meins, \& Fernyhough, 2010; e.g., Morin et al., 2018; Wallace, Peng, \& Williams, 2017)\footnote{We use the terms ``inner speech'' and ``inner voice'' interchangeably, but we are not committed to the view that inner speech has all the same auditory and articulatory features as overt speech (Fernyhough \& Borghi, 2023, for a recent overarching review; Langland-Hassan, 2018). Importantly, inner speech displays variation both in terms of its form (e.g., dialogic vs.~condensed) and modality (e.g., inner speech as hearing a voice vs.~experiencing the imagined articulation of speech) (Alderson-Day, Mitrenga, Wilkinson, McCarthy-Jones, \& Fernyhough, 2018; Grandchamp et al., 2019; Gregory, 2016; Perrone-Bertolotti et al., 2014). There is evidence that the different modalities of inner speech involve different neural and cognitive mechanisms (e.g., Nalborczyk et al., 2023; Tian, Zarate, \& Poeppel, 2016).}. But not everyone experiences inner speech. This is attested by personal narratives such as `What it's like living without an inner voice' (Soloducha, 2020) and `People With No Internal Monologue Explain What It's Like In Their Head' (Felton, 2020), as well as more systematic investigations both targeting variation in inner speech (Alderson-Day et al., 2018; Brinthaupt, 2019; Hurlburt, Heavey, \& Kelsey, 2013) and auditory imagery, which has sometimes been used as a proxy for inner speech (Dawes, Keogh, Andrillon, \& Pearson, 2020; Hinwar \& Lambert, 2021).

\hypertarget{the-present-study}{%
\subsection{The Present Study}\label{the-present-study}}

We recruited participants differing in subjectively reported inner speech and tested them on four behavioral tasks. These tasks were chosen based on prior theoretical claims that suggested performance on them may differ as a function of inner speech. First, just as visual imagery has been predicted (and sometimes found) to be linked to visual memory, we tested whether inner speech predicted memory for verbal material. We focused on memory for sets of words that were either phonologically similar and orthographically different or orthographically similar and phonologically different. Less inner speech was predicted to be associated with poorer overall memory for verbal material, but to the extent that phonological similarity creates memory confusion (Baddeley, 1966; Murray, 1968), less inner speech may be associated with a reduced phonological similarity effect. Second, participants completed a rhyme judgment task: participants saw pairs of images and needed to indicate whether their names rhymed or not. We reasoned that although participants with low inner speech would have no trouble naming the objects, a reduced reliance on inner speech would make it harder to compare the names in memory -- necessary for making a rhyme judgment (Geva, Bennett, Warburton, \& Patterson, 2011; Langland-Hassan, Faries, Richardson, \& Dietz, 2015). Third, there is substantial evidence that inner speech is often recruited for behavioral control when participants have to switch between different tasks (Baddeley, Chincotta, \& Adlam, 2001; Emerson \& Miyake, 2003; Laurent et al., 2016; Miyake, Emerson, Padilla, \& Ahn, 2004). For example, when asked to switch between adding and subtracting numbers, participants show a selective impairment if they undergo articulatory suppression, but no such impairment is found if the cues are exogenously provided, e.g., a symbol or color cue is used to inform participants whether they should add or subtract (see Nedergaard, Wallentin, \& Lupyan, 2022, for a systematic review of verbal interference effects). We reasoned that people who do not habitually use inner speech might be selectively impaired when they have to rely on self-generated cues to keep track of which task they should be doing. On the other hand, it is possible that they have learned to rely on other strategies in which case no difference would be found. Our fourth task involved examining category effects in perception. There is considerable evidence that language induces more categorical representations from basic perception onward (Forder \& Lupyan, 2019; Perry \& Lupyan, 2014; e.g., Winawer et al., 2007). In a study examining the effects of conceptual categories, Lupyan, Thompson-Schill, and Swingley (2010) showed that - controlling for visual differences - people's ability to tell whether two stimuli were physically the same was affected by the categorical status of those stimuli. For example, it took longer to distinguish two cats than an equally visually similar cat and dog. We wondered whether such category effects, insofar as they may be in part induced by feedback from verbal labels, may be reduced in people with less inner speech. For all four experiments, we were also interested in whether performance differed by whether participants reported talking out loud during the task.

\hypertarget{open-practices-statement}{%
\section{Open Practices Statement}\label{open-practices-statement}}

The experiment code, materials, data, and analysis scripts can be accessed at \url{https://anonymous.4open.science/r/anendophasia-6F15}. The studies were not pre-registered.

\hypertarget{methods}{%
\section{Methods}\label{methods}}

\hypertarget{measurement-of-inner-speech}{%
\subsection{Measurement of inner speech}\label{measurement-of-inner-speech}}

We measured subjectively experienced inner speech using a previously developed and validated Internal Representations Questionnaire (IRQ) (Roebuck \& Lupyan, 2020). This questionnaire is broadly similar to other surveys of inner-speech (e.g., the General Inner Speech Questionnaire: Racy, Morin, \& Duhnych, 2020; the Self-Talk Scale: Brinthaupt, Hein, \& Kramer, 2009; the Varieties of Inner Speech Questionnaire: McCarthy-Jones \& Fernyhough, 2011), and its verbal factor is most closely related to dialogic inner-speech as measured by the Varieties of Inner Speech Questionnaire (r\textasciitilde.7). Two advantages of the IRQ are that its inner speech questions are more inclusive than those on the other scales and the same instrument can be used to assess other individual differences such as visual and orthographic imagery. As is true of other scales, the IRQ measures propensities rather than abilities. Tools to measure inner speech \emph{abilities} have also been developed (see e.g. Geva \& Warburton, 2019) but differ from the IRQ in focusing solely on phonological properties of inner speech.

\hypertarget{participants}{%
\subsection{Participants}\label{participants}}

We recruited participants online who had previously completed the IRQ as part of unrelated studies, contacting participants with verbal factor scores \textless= 3.5 (bottom 30\%-ile) or \textgreater= 4.25 (top 20\%-ile) on the Verbal factor of the questionnaire which is largely centered on propensity to experience and rely on inner speech. For example, one item with a high loading on the Verbal factor was `I think about problems in my mind in the form of a conversation with myself'. One item with a high loading on the Visual factor was `I often enjoy the use of mental pictures to reminisce' (see Supplemental Materials for all verbal factor items). The percentile cut-offs were asymmetric because the distribution in verbal scores on the IRQ is negatively skewed. Recruiting for example the top and bottom quartiles would have resulted in a ``low inner speech'' group who had moderate amounts of reported inner speech. The final sample included participants from the bottom 20\%-ile and the top 29\%-ile (see histogram with cutoff values in Supplemental Materials).\footnote{Due to a recruiting error, three participants recruited for the more inner speech group had verbal scores slightly below 4.25 (4.17).} We received ethical approval from {[}redacted{]}. Ten participants were excluded for responding randomly, missing at least one experiment, or clearly not complying with task instructions. Our final sample included 47 participants with relatively high verbal factor scores on the IRQ and 46 participants with relatively low verbal factor scores. The two groups were balanced in terms of age, gender, education level, dyslexia, and first language. See Table \ref{tab:demographics}. Due to a technical error, demographic data for one participant in the low inner speech group was missing. We were interested in detecting medium-to-large effects. Our sample size allows us to detect effect sizes of approximately .6 at 80\% power or .7 at 91\% power (two-tailed t-test of mean difference between two independent groups). Interaction effects and their associated effect sizes should be interpreted with caution due to increased error rates and potentially distorted effect size estimates. Our repeated measures approach in part ameliorates these issues by modelling within-participant and within-item variation.

\begin{table}[!h]

\caption{\label{tab:demographics}Comparisons of demographic characteristics of the group with more inner speech and the group with less inner speech.}
\centering
\fontsize{8}{10}\selectfont
\begin{tabu} to \linewidth {>{\raggedright}X>{\raggedright}X>{\raggedright}X>{\raggedright}X}
\toprule
\textbf{Measure} & \textbf{More inner speech} & \textbf{Less inner speech} & \textbf{Test for difference}\\
\midrule
Age & Mean = 36.91; Median = 37;
range = 18-67 & Mean = 37.56; Median = 39;
range = 18-70 & t(88.43) = -0.19; p = .85\\
Gender & 22 female, 25 male & 19 female, 26 male & $\chi^{2}$(1) = 0.05; p = .82\\
Native English-speaker & 47 native speakers,
0 non-native speakers & 41 native speakers,
4 non-native speakers & $\chi^{2}$(1) = 2.49; p = .11\\
Dyslexia & 46 non-dyslexic,
1 self-diagnosed & 44 non-dyslexic,
1 self-diagnosed & $\chi^{2}$(1) < 0.01; p > .99\\
Education level & 12 high school diploma,
14 some college - no degree,
6 associate's degree,
14 bachelor's degree,
1 master's degree & 1 less than high school,
14 high school diploma,
8 some college - no degree,
7 associate's degree,
11 bachelor's degree,
2 master's degree,
2 PhD, law, or medical degree & t(84.46) = -0.23; p = .82\\
\bottomrule
\end{tabu}
\end{table}

\hypertarget{method-verbal-working-memory}{%
\subsection{Method: Verbal working memory}\label{method-verbal-working-memory}}

\hypertarget{materials-and-procedure}{%
\subsubsection{Materials and procedure}\label{materials-and-procedure}}

We used word sets from Baddeley (1966) which were designed to vary in phonological and orthographic similarity, while holding constant other psycholinguistic factors. The phonologically-similar set contained the words ``bought'', ``sort'', ``taut'', ``caught'', and ``wart''. The orthographically similar set contained the words ``rough'', ``cough'', ``through'', ``dough'', and ``bough''. The control set contained the words ``plea'', ``friend'', ``sleigh'', ``row'', and ``board''. On a given trial, participants saw five written words in random order from one of the sets. The words were presented sequentially, see Figure \ref{fig:verbwm-procedure}. After the last word, participants were asked to type the five words they just saw in the order they saw them. Participants began the task by completing two practice trials with full feedback (correct/incorrect and the stimulus words -- drawn from a different set than the ones used in the real experiment -- shown in order). Participants then performed 24 trials in total with eight trials from each of the three word sets. The order of both set type and words within a trial were randomized. There was no limit to how long participants could spend on reproducing the words on a given trial.

\newpage

\begin{figure}[!ht]
\includegraphics[width=1\linewidth]{../figures/verbwm} \caption{A schematic of the procedure, showing a trial with phonologically-related words. Each trial had five words.}\label{fig:verbwm-procedure}
\end{figure}

\hypertarget{method-rhyme-judgments}{%
\subsection{Method: Rhyme judgments}\label{method-rhyme-judgments}}

\hypertarget{materials-and-procedure-1}{%
\subsubsection{Materials and procedure}\label{materials-and-procedure-1}}

We constructed a set of rhyme pairs with 20 orthographic pairs (e.g., ``sock'' and ``clock'') and 20 non-orthographic pairs (e.g., ``drawer'' and ``door''). See Supplemental Materials for the full set of images, associated words, and name agreement scores. The images were selected from the MultiPic database (Duñabeitia et al., 2018) and from Rossion and Pourtois (2004) because those image sets contained simple images (objects with no background) that had relatively high name agreement. On each trial, participants saw two images of items presented simultaneously and were asked to judge whether the names of the items rhymed or not. Participants completed 60 rhyme judgments in randomized order (20 orthographic rhymes, 20 non-orthographic rhymes, and 20 no-rhyme control trials). There was a 5000 ms response deadline. See Figure \ref{fig:rhyme-procedure}.

\newpage

\begin{figure}[!ht]
\includegraphics[width=1\linewidth]{../figures/rhyme} \caption{A sketch of a rhyme judgment trial. The stimuli here exemplify an orthographic rhyme – "bone" and "cone" – and the correct answer would therefore be "Rhyme".}\label{fig:rhyme-procedure}
\end{figure}

\hypertarget{method-task-switching}{%
\subsection{Method: Task switching}\label{method-task-switching}}

\hypertarget{materials-and-procedure-2}{%
\subsubsection{Materials and procedure}\label{materials-and-procedure-2}}

On each block, participants were shown 30 randomly selected integers between 13 and 96 and asked to add or subtract 3 from each. All participants completed five blocks beginning with blocked addition or blocked subtraction, followed by (in a counterbalanced order) a block where problems alternated between addition and subtraction with the operation marked by color (red/blue), marked with a symbol (+/-), or not marked. The unmarked block required participants to remember which operation they had just done. In the switching conditions, a response counted as correct if it was the correct arithmetic and if the operation was switched from the previous trial (from addition to subtraction or vice versa). See Figure \ref{fig:task-switch-procedure}.

\begin{figure}[!ht]
\includegraphics[width=1\linewidth]{../figures/taskswitch} \caption{A sketch of the three switched conditions in the task switching experiment. Figure A shows four color-cued switch trials with correct answers, Figure B shows four symbol-cued switch trials with correct answers, and Figure C shows four un-cued switch trials with correct answers.}\label{fig:task-switch-procedure}
\end{figure}

\hypertarget{method-samedifferent-judgments}{%
\subsection{Method: Same/different judgments}\label{method-samedifferent-judgments}}

\hypertarget{materials-and-procedure-3}{%
\subsubsection{Materials and procedure}\label{materials-and-procedure-3}}

This experiment used three different black silhouettes of cats and three different black silhouettes of dogs.
Participants completed two blocked conditions: making physical identity judgments (same means physically identical) and making category judgments (same means same category). We are only interested in the physical identity judgments here. Participants completed 200 total trials and received feedback after incorrect responses (`incorrect' in red font). See Figure \ref{fig:same-diff-procedure}.

\begin{figure}[!ht]
\includegraphics[width=1\linewidth]{../figures/samedifferent} \caption{A sketch of the two conditions of the category judgment experiment. On Figure A, we see a correct category judgment trial where the participant responds that the cat and dog silhouettes represent different animals. On Figure B, we see an incorrect identity judgment trial where the participant responds that the two dogs are identical.}\label{fig:same-diff-procedure}
\end{figure}

\hypertarget{method-questionnaire}{%
\subsection{Method: Questionnaire}\label{method-questionnaire}}

After completing the four experiments, participants answered a series of questions about their experience with inner speech (e.g.~`How often do you have songs stuck in your head?' and `Do you ever rehearse a conversation before you have it in real life where you simulate what you will say and how the other person will respond?') and completed the Varieties of Inner Speech Questionnaire-Revised (VISQ-R) (Alderson-Day et al., 2018). The VISQ-R measures the extent to which inner speech is experienced as dialogic (e.g., `I talk back and forward to myself in my mind about things') and condensed (e.g.~`My thinking in words is shortened compared to my normal out-loud speech') as well as whether the participant experiences the voices of other people. The questionnaire also measures the perceived functions of inner speech through asking about inner speech as an evaluative and regulatory tool (e.g., `I think in inner speech about what I have done, and whether it was right or not'). See Supplemental Materials for the full set of custom questions.

\hypertarget{data-analysis}{%
\subsection{Data analysis}\label{data-analysis}}

All analyses were conducted in R version 4.1.3 (R Core Team, 2022). Participants and items (where appropriate) were modeled as random intercepts; random slopes were included for within-subject factors unless it prevented convergence. All predictors were centered. Reaction times were log-transformed to yield a more normal distribution. Accuracies were modeled using logistic regression. For ease of interpretation, the figures show the two inner speech groups as distinct, but all the statistical models use verbal score (average score on the verbal representation items on the Internal Representations Questionnaire) as a continuous predictor. Error bars on all figures represent within-participant 95\% confidence intervals around the mean (adjusted for repeated measures). All four experiments were conducted using custom-written software with the JavaScript package jsPsych version 6 (De Leeuw, 2015), and data and code can be found at \url{https://anonymous.4open.science/r/anendophasia-6F15}.

\hypertarget{results}{%
\section{Results}\label{results}}

\hypertarget{verbal-working-memory}{%
\subsection{Verbal working memory}\label{verbal-working-memory}}

In the verbal working memory experiment, we tested whether the number of words that participants were able to correctly recall (the dependent variable) was predicted by participants' verbal score on the IRQ and the type of word set (control set, orthographic similarity set, phonological similarity set).

\hypertarget{descriptive-statistics-by-group-verbal-working-memory}{%
\subsubsection{Descriptive statistics by group: Verbal working memory}\label{descriptive-statistics-by-group-verbal-working-memory}}

Participants with more inner speech recalled more words correctly. This advantage was evident both when we scored only correctly ordered responses as correct as well as when we scored correctly recalled items regardless of their position (see Table \ref{tab:phonsim-desc-table} and Figure \ref{fig:phonsim-score-desc}).

\begin{table}

\caption{\label{tab:phonsim-desc-table}Descriptive statistics by group in the verbal working memory experiment.}
\centering
\fontsize{8}{10}\selectfont
\begin{tabular}[t]{>{\raggedright\arraybackslash}p{12em}>{\raggedright\arraybackslash}p{12em}>{\raggedleft\arraybackslash}p{5em}>{\raggedright\arraybackslash}p{5em}>{\raggedleft\arraybackslash}p{5em}>{\raggedright\arraybackslash}p{5em}}
\toprule
\textbf{Group} & \textbf{Word set} & \textbf{Score (item and position)} & \textbf{95\% CI (item and position)} & \textbf{Score (item only)} & \textbf{95\% CI (item only)}\\
\midrule
More inner speech & Control set & 4.19 & ±0.13 & 4.51 & ±0.08\\
More inner speech & Orthographic similarity set & 3.72 & ±0.14 & 4.18 & ±0.1\\
More inner speech & Phonological similarity set & 3.43 & ±0.16 & 4.11 & ±0.1\\
Less inner speech & Control set & 3.69 & ±0.15 & 4.17 & ±0.11\\
Less inner speech & Orthographic similarity set & 3.52 & ±0.15 & 4.10 & ±0.11\\
\addlinespace
Less inner speech & Phonological similarity set & 3.02 & ±0.15 & 3.81 & ±0.11\\
\bottomrule
\end{tabular}
\end{table}

\begin{figure}
\centering
\includegraphics{anendophasia_paper_psychsci_files/figure-latex/phonsim-score-desc-1.pdf}
\caption{\label{fig:phonsim-score-desc}Score on the verbal working memory task by word set.}
\end{figure}

\hypertarget{statistical-models-verbal-working-memory}{%
\subsubsection{Statistical models: Verbal working memory}\label{statistical-models-verbal-working-memory}}

Participants remembered phonologically similar words significantly worse (M = 3.22) than orthographically-similar words (M = 3.62) (\(\beta\) = -0.72; SE = 0.08; t = -8.84; p \textless{} .001; standardized \(\beta\) = -0.22 {[}-0.33,-0.11{]}) which were in turn remembered worse than the dissimilar words (M = 3.94) (\(\beta\) = -0.33; SE = 0.08; t = -3.98; p \textless{} .001; standardized \(\beta\) (effect size) = -0.47 {[}-0.57,-0.36{]}). Collapsing across the three types of word lists, greater inner speech was associated with better performance (\(\beta\) = 0.27; SE = 0.10; t = 2.60; p = .011; standardized \(\beta\) (effect size) = 0.17 {[}0.04,0.31{]}). This effect remained significant when we ignore the recalled order of the words, counting only whether they recalled the correct words (\(\beta\) = 0.19; SE = 0.08; t = 2.57; p = .012; standardized \(\beta\) (effect size) = 0.18 {[}0.04,0.32{]}). There were no interaction effects (all p \textgreater{} .10), although numerically, the effect of inner speech was smallest for orthographically similar words (see Figure \ref{fig:phonsim-score-desc}).

\hypertarget{strategies-verbal-working-memory}{%
\subsubsection{Strategies: Verbal working memory}\label{strategies-verbal-working-memory}}

\begin{figure}
\centering
\includegraphics{anendophasia_paper_psychsci_files/figure-latex/phon-sim-TOL-fig-1.pdf}
\caption{\label{fig:phon-sim-TOL-fig}Verbal working memory performance by whether participants reported talking out loud to help them remember or not.}
\end{figure}

The groups with more and less inner speech were similar in their reported use of talking out loud as a strategy for remembering the words: 10 out of 47 in the group with more inner speech; 13 out of 46 in the group with less inner speech (\(\chi^2\)(1) = 0.29, p = .59). Nevertheless, talking out loud was associated with performance in different ways between the two groups (see Figure \ref{fig:phon-sim-TOL-fig}). As the figure indicates, there was an interaction effect between talking out loud and verbal score on recall (\(\beta\) = -0.50; SE = 0.23; t = -2.19; p = .031; standardized \(\beta\) (effect size) = -0.14 {[}-0.26,-0.01{]}). Participants with less inner speech who reported using overt language during the task performed similarly to participants with more inner speech, suggesting that what mattered for performance was the use of speech, either covert or overt.

\hypertarget{rhyme-judgments}{%
\subsection{Rhyme judgments}\label{rhyme-judgments}}

In the rhyme judgment experiment, we tested whether the speed and accuracy with which participants made rhyme judgments (the dependent variables) were predicted by participants' verbal score on the IRQ and the type of rhyme (orthographic rhyme, non-orthographic rhyme, and no rhyme). We also tested whether participants' rhyme judgment performance differed by whether they reported talking out loud to remember the words. Five image pairs of rhyming objects -- bin/chin, cab/crab, rake/cake, wave/cave, and park/shark -- were incorrectly judged to not rhyme on at least half the trials. This was most likely because participants did not name one or both of the images with the intended names (mean agreement rating for these 10 images = 0.58; range = 0.05 to 1). We therefore excluded these trials from further analysis. In addition, we trimmed reaction times below 200 ms (68 trials, 1.4\%).

\hypertarget{descriptive-statistics-by-group-rhyme-judgments}{%
\subsubsection{Descriptive statistics by group: Rhyme judgments}\label{descriptive-statistics-by-group-rhyme-judgments}}

Participants who reported having more inner speech were numerically both faster and more accurate than participants who reported having less inner speech on all three types of trials, see Table \ref{tab:rhyme-desc-table}, and Figure \ref{fig:rhyme-desc}.

\begin{table}

\caption{\label{tab:rhyme-desc-table}Descriptive statistics on rhyming accuracy and reaction time by group and by rhyme type.}
\centering
\fontsize{8}{10}\selectfont
\begin{tabular}[t]{>{\raggedright\arraybackslash}p{12em}>{\raggedright\arraybackslash}p{12em}>{\raggedleft\arraybackslash}p{5em}>{\raggedright\arraybackslash}p{5em}>{\raggedleft\arraybackslash}p{5em}>{\raggedright\arraybackslash}p{5em}}
\toprule
\textbf{Group} & \textbf{Type of rhyme trial} & \textbf{Reaction time (ms)} & \textbf{95\% CI (reaction time)} & \textbf{Accuracy} & \textbf{95\% CI (accuracy)}\\
\midrule
More inner speech & Non-orthographic rhyme & 1853 & ±51 & 83.75 & ±2.81\\
More inner speech & No rhyme & 1931 & ±53 & 98.45 & ±1.13\\
More inner speech & Orthographic rhyme & 1719 & ±55 & 91.99 & ±2.37\\
Less inner speech & Non-orthographic rhyme & 1976 & ±54 & 77.75 & ±3.19\\
Less inner speech & No rhyme & 2027 & ±60 & 95.57 & ±1.65\\
\addlinespace
Less inner speech & Orthographic rhyme & 1859 & ±60 & 84.48 & ±3.16\\
\bottomrule
\end{tabular}
\end{table}

\begin{figure}
\centering
\includegraphics{anendophasia_paper_psychsci_files/figure-latex/rhyme-desc-1.pdf}
\caption{\label{fig:rhyme-desc}Reaction time and accuracy across groups by rhyme type.}
\end{figure}

\hypertarget{statistical-models-rhyme-judgments}{%
\subsubsection{Statistical models: Rhyme judgments}\label{statistical-models-rhyme-judgments}}

Participants took longer to make rhyme judgments on no-rhyme trials (M = 1981) compared to orthographic trials (M = 1730) (\(\beta\) = 0.12; SE = 0.04; t = 3.01; p = .005; standardized \(\beta\) (effect size) = 0.1 {[}0.04,0.17{]}). Non-orthographic trials (M = 1823) did not differ significantly from orthographic trials (\(\beta\) = 0.05; SE = 0.04; t = 1.18; p = .27; standardized \(\beta\) (effect size) = 0.04 {[}-0.03,0.11{]}). Higher name agreement was associated with faster RTs (\(\beta\) = -0.04; SE = 0.02; t = -2.22; p = .029; standardized \(\beta\) (effect size) = -0.03 {[}-0.06,0{]}). Reported inner speech had no effect on speed of correct rhyme judgments (\(\beta\) = -0.02; SE = 0.02; t = -0.82; p = .42; standardized \(\beta\) (effect size) = -0.01 {[}-0.05,0.02{]}). There were no interactions between rhyme type and inner speech (both p's \textgreater{} .29) or between inner speech and the effect of name agreement on accuracy (p \textgreater{} .97).

Participants were more accurate when judging no-rhyme trials as not rhyming (M = 97.03\%) than on orthographic rhyme judgments (M = 88.28\%) (\(\beta\) = 1.67; SE = 0.32; z = 5.15; p \textless{} .001; standardized \(\beta\) (effect size) = 1.67 {[}1.04,2.31{]}) and were less accurate on non-orthographic rhyme judgments (M = 80.8\%) than on orthographic rhyme judgments (\(\beta\) = -0.59; SE = 0.28; z = -2.07; p = .038; standardized \(\beta\) (effect size) = -0.59 {[}-1.14,-0.03{]}). Importantly, a higher verbal score was associated with greater accuracy (\(\beta\) = 0.34; SE = 0.12; z = 2.81; p = .005; standardized \(\beta\) (effect size) = 0.34 {[}0.1,0.58{]}). Name agreement did not affect accuracy (p \textgreater{} .12). There were no significant interactions between rhyme type and inner speech (both p \textgreater{} .21) or between inner speech and effect of name agreement on accuracy (p = .58).

\hypertarget{strategies-rhyme-judgments}{%
\subsubsection{Strategies: Rhyme judgments}\label{strategies-rhyme-judgments}}

Asked about their strategies, similar proportions of participants in both groups reported naming the pictures out loud: 23 out of 47 in the higher inner speech group and 21 out of 46 in the lower inner speech group (\(\chi^2\)(1) = 0.01, p = .91). We observed a similar interaction here as with the memory task (compare Figure \ref{fig:phon-sim-TOL-fig} and Figure \ref{fig:rhyme-TOL}). Saying the words out loud reduced the accuracy disadvantage associated with having less inner speech for both non-orthographic rhymes (\(\beta\) = -0.75; SE = 0.34; z = -2.24; p = .025; standardized \(\beta\) (effect size) = -0.37 {[}-0.7,-0.05{]}) and orthographic rhymes (\(\beta\) = -0.78; SE = 0.36; z = -2.19; p = .028; standardized \(\beta\) (effect size) = -0.39 {[}-0.74,-0.04{]}), suggesting that once again speech use - whether covert or overt - is associated with higher accuracy.

\begin{figure}
\centering
\includegraphics{anendophasia_paper_psychsci_files/figure-latex/rhyme-TOL-1.pdf}
\caption{\label{fig:rhyme-TOL}Reaction time and accuracy by whether participants indicated that they had talked out loud to make the rhyme judgments.}
\end{figure}

\hypertarget{task-switching}{%
\subsection{Task switching}\label{task-switching}}

In the task switching experiment, we tested whether the speed and accuracy of performing simple arithmetic operations (adding and subtracting) were predicted by participants' reported inner speech (verbal score on the IRQ) as a function of how they were cued to alternate between two operations: addition and subtraction when cued to the correct operation by a symbol, by a color, or having to rely on their memory of which operation they just did. We excluded trials with RTs over 10 seconds (0.5 \% of trials). We also recalculated the accuracy measure so that a failure to switch did not render all subsequent trials incorrect as long as the participant proceeded to switch appropriately and obtain the arithmetically correct answer.

\hypertarget{descriptive-statistics-task-switching}{%
\subsubsection{Descriptive statistics: Task switching}\label{descriptive-statistics-task-switching}}

As can be seen from Table \ref{tab:task-switch-desc-table} and Figure \ref{fig:task-switch-desc-fig}, accuracy was high in all conditions, and reaction times were comparable across the two groups of participants.

\begin{table}[!h]

\caption{\label{tab:task-switch-desc-table}Descriptive statistics of reaction time and accuracy on the task switching experiment.}
\centering
\fontsize{8}{10}\selectfont
\begin{tabular}[t]{>{\raggedright\arraybackslash}p{12em}>{\raggedright\arraybackslash}p{12em}>{\raggedleft\arraybackslash}p{5em}>{\raggedright\arraybackslash}p{5em}>{\raggedleft\arraybackslash}p{5em}>{\raggedright\arraybackslash}p{5em}}
\toprule
\textbf{Group} & \textbf{Condition} & \textbf{Mean reaction time (ms)} & \textbf{95\% CI (reaction time)} & \textbf{Accuracy} & \textbf{95\% CI (accuracy)}\\
\midrule
More inner speech & Blocked
addition & 2287 & ±47 & 97.94 & ±0.83\\
More inner speech & Color-cued
switch & 2775 & ±62 & 95.64 & ±1.16\\
More inner speech & Blocked
subtraction & 2528 & ±54 & 97.65 & ±0.89\\
More inner speech & Symbol-cued
switch & 2564 & ±54 & 97.72 & ±0.86\\
More inner speech & Un-cued
switch & 2679 & ±59 & 94.59 & ±1.29\\
\addlinespace
Less inner speech & Blocked
addition & 2312 & ±46 & 98.32 & ±0.76\\
Less inner speech & Color-cued
switch & 2781 & ±63 & 95.08 & ±1.26\\
Less inner speech & Blocked
subtraction & 2573 & ±55 & 97.80 & ±0.88\\
Less inner speech & Symbol-cued
switch & 2640 & ±56 & 96.72 & ±1.03\\
Less inner speech & Un-cued
switch & 2710 & ±64 & 93.19 & ±1.47\\
\bottomrule
\end{tabular}
\end{table}

\begin{figure}
\centering
\includegraphics{anendophasia_paper_psychsci_files/figure-latex/task-switch-desc-fig-1.pdf}
\caption{\label{fig:task-switch-desc-fig}Reaction time and accuracy across conditions in the task switching experiment.}
\end{figure}

\hypertarget{statistical-models-task-switching}{%
\subsubsection{Statistical models: Task switching}\label{statistical-models-task-switching}}

Participants responded less accurately in the symbol-cued switch condition (M = 97.2\%), in the color-cued switch condition (M = 95.4\%), and in the un-cued switch condition (M = 93.9\%) compared to the blocked addition condition (M = 98.1\%) (addition versus symbol-cue: \(\beta\) = -0.42; SE = 0.18; z = -2.32; p = .020; standardized \(\beta\) (effect size) = -0.42 {[}-0.77,-0.07{]}; addition versus color-cue: \(\beta\) = -0.97; SE = 0.17; z = -5.84; p \textless{} .001; standardized \(\beta\) (effect size) = -0.97 {[}-1.3,-0.65{]}; addition versus un-cued: \(\beta\) = -1.27; SE = 0.16; z = -7.92; p \textless{} .001; standardized \(\beta\) (effect size) = -1.27 {[}-1.59,-0.96{]}). Accuracy did not differ between blocked subtraction (M = 97.7\%) and blocked addition (p = .24). More inner speech was not associated with different accuracy (p = .55) and there were no interaction effects between inner speech and block-type (all p's \textgreater{} .07). Numerically, verbal score interacted with the un-cued condition and cancelled out the very slight (non-significant) reaction time advantage of a higher verbal score.

Participants responded faster in the blocked addition condition (M = 2300 ms) compared to the subtraction condition (M = 2550 ms) (\(\beta\) = 0.09; SE = 0.01; t = 8.41; p \textless{} .001; standardized \(\beta\) (effect size) = 0.08 {[}0.06,0.1{]}), the symbol-cued switch condition (M = 2601 ms) (\(\beta\) = 0.12; SE = 0.01; t = 9.69; p \textless{} .001; standardized \(\beta\) (effect size) = 0.1 {[}0.08,0.13{]}), the color-cued switch condition (M = 2778 ms) (\(\beta\) = 0.19; SE = 0.02; t = 12.23; p \textless{} .001; standardized \(\beta\) (effect size) = 0.17 {[}0.14,0.19{]}), and the un-cued switch condition (M = 2694 ms) (\(\beta\) = 0.15; SE = 0.02; t = 9.39; p \textless{} .001; standardized \(\beta\) (effect size) = 0.13 {[}0.11,0.16{]}). More reported inner speech did not predict reaction times (p = .81), and there were no interaction effects (all p's \textgreater{} .51).

\hypertarget{strategies-task-switching}{%
\subsubsection{Strategies: Task switching}\label{strategies-task-switching}}

There was no significant difference between how many participants with more inner speech (20 out of 47) and how many participants with less inner speech (13 out of 46) reported that they had talked to themselves out loud during the task switching experiment (\(\chi^2\)(1) = 1, p = .32). There were no obvious differences between the effects that talking out loud had on these two groups (see accuracy and reaction time Figure \ref{fig:task-switch-TOL}).

\begin{figure}
\centering
\includegraphics{anendophasia_paper_psychsci_files/figure-latex/task-switch-TOL-1.pdf}
\caption{\label{fig:task-switch-TOL}Reaction time (ms) and accuracy in the task switching experiment by whether participants reported talking out loud to remember the correct rule or not.}
\end{figure}

\hypertarget{samedifferent-judgments}{%
\subsection{Same/different judgments}\label{samedifferent-judgments}}

In the same/different judgment experiment, we tested whether the speed with which participants made correct same/different judgments was predicted by participants' verbal score on the IRQ and the type of judgment (same category of animal or same image). We excluded trials with RTs above 5 seconds (0.7 \%) and below 200 ms (0.07 \%). Overall accuracy was high, 95.53, and did not differ between the two inner speech groups (95.58 \%) and the group with less inner speech (95.48 \%). In subsequent RT analyses, we only include correct trials.

\hypertarget{statistical-models-samedifferent-judgments}{%
\subsubsection{Statistical models: Same/different judgments}\label{statistical-models-samedifferent-judgments}}

\begin{figure}
\centering
\includegraphics{anendophasia_paper_psychsci_files/figure-latex/samediff-key-comp-1.pdf}
\caption{\label{fig:samediff-key-comp}Reaction time on identity trials where the correct response was `DIFFERENT' either because the two silhouettes were from different categories or different images from the same category.}
\end{figure}

The key test for this experiment was whether the two groups behaved differently when giving correct `DIFFERENT' responses on identity trials when the two images belonged to the same category. That is, we expected participants with more inner speech to be slower to make correct `DIFFERENT' responses when both stimuli where from the same category but physically different (i.e., \(dog_1\) versus \(dog_2\)). Within-category trials were generally associated with significantly slower reaction times (M = 923 ms) than between-category trials (M = 843 ms) (\(\beta\) = -0.08; SE = 0.01; t = -7.71; p \textless{} .001; standardized \(\beta\) (effect size) = -0.09 {[}-0.11,-0.06{]})). See Figure \ref{fig:samediff-key-comp}. However, there was no interaction between level of inner speech and category-type: (interaction effect: \(\beta\) = 0.00; SE = 0.01; t = -0.06; p = .95; standardized \(\beta\) (effect size) = 0 {[}-0.02,0.02{]}).

\hypertarget{strategies-samedifferent-judgments}{%
\subsubsection{Strategies: Same/different judgments}\label{strategies-samedifferent-judgments}}

There was no significant difference between how many participants with more inner speech (9 out of 47) and how many participants with less inner speech (4 out of 46) reported that they had talked to themselves out loud during the task (\(\chi^2\)(1) = 1.33, p = .25). There were no differences between the effects that talking out loud had on these two groups.

\hypertarget{intertask-correlations}{%
\subsection{Intertask correlations}\label{intertask-correlations}}

In addition to finding (or not finding) differences in task performance as a function of inner speech, it is often informative to see whether correlations between tasks and conditions show a different pattern in people with more vs.~less inner speech (Keogh, Wicken, \& Pearson, 2021). See Figure \ref{fig:total-cor-contrast} for a visualization of how performance on the tasks correlated within the participant groups with more or less inner speech.The dark-blue clusters near the diagonal show that for both groups performance \emph{within} tasks (e.g., RTs on the different types of task-switch trials) was strongly correlated, and similarly so for both groups. When it comes to relationships \emph{between} tasks, however, we found several intriguing differences: Participants with less inner speech showed a positive correlation between verbal recall accuracy and non-orthographic rhyme accuracy (r=.48). This group also showed moderate correlations (r's between .3 and .5) between uncued task-switch accuracy and various measures of verbal recall accuracy. In contrast, participants with more inner speech showed weaker relationships between these measures (r's between .16 and .30).

\begin{figure}[!ht]
\includegraphics[width=1\linewidth]{../figures/cor_plot_contrast_v2} \caption{Intertask correlations. The upper triangle shows correlations for the group with more inner speech; the lower triangle shows correlations for the group with less inner speech.
Colored squares represent significant correlations at p < .01 (|r|>.38 given the sample size).}\label{fig:total-cor-contrast}
\end{figure}

\newpage

\hypertarget{questionnaire-measures}{%
\subsection{Questionnaire measures}\label{questionnaire-measures}}

Responses to many of the included questions differed substantially as a function of inner speech\footnote{Data from one participant was missing so we report questionnaire data from 47 participants with more inner speech and 45 participants with less inner speech.}. For reasons of space, however, we only report a few selected ones here (see Supplemental Materials for further correlations). The questions with the clearest differences concerned rehearsing and revising conversations where the participants with more inner speech reported doing so much more often than the participants with less inner speech did (revise past conversation: t(87.95) = 5.93; p \textless{} .001; practice future conversation: t(89.33) = 5.33; p \textless{} .001). Of the VISQ factors, the IRQ verbal representation score was mostly related to the dialogicality of inner speech (r(90) = .70; p \textless{} .001).

Participants who reported more inner speech estimated that more people experience their thoughts in the form of a conversation with themselves (\(\beta\) = 5.08; SE = 2; t = 2.55; p = .013; standardized \(\beta\) (effect size) = 0.26 {[}0.06,0.46{]}) and that more people hear words in their ``mind's ear'' when they read (\(\beta\) = 5.09; SE = 2.07; t = 2.46; p = .016; standardized \(\beta\) (effect size) = 0.25 {[}0.05,0.45{]}). They did not, however, estimate that more people were able to see vivid images in their ``mind's eye'' (\(\beta\) = 1.17; SE = 2.25; t = 0.52; p = .61; standardized \(\beta\) (effect size) = 0.05 {[}-0.15,0.26{]}).

\hypertarget{discussion}{%
\section{Discussion}\label{discussion}}

Our study is, to our knowledge, the first to conduct a systematic investigation of whether differences in inner speech have behavioral consequences. Participants who report experiencing less inner speech (our sample targeted those at \textless{} 20\%ile of the verbal score on the Internal Representations Questionnaire) performed worse when judging whether the names of two images rhymed, and they had poorer verbal working memory. Interestingly, in both the rhyming experiment and the verbal working memory experiment, performance differences between the two groups disappeared when participants reported talking out loud to solve the problems, suggesting that the efficacy of using covert and overt speech in these cases were equivalent. Inner speech differences did not predict performance in task switching, suggesting that while inner speech can be used as a behavioral self-cue, other and equally effective strategies may be available. Lastly, categorical effects on perceptual discrimination were similar for the two groups suggesting either that the categorical effects in such tasks are not language-based, or that the speeded nature of such tasks lessens reliance on inner speech (and language more generally). Examination of intertask correlations suggested that participants with less inner speech were more likely to rely on a common mechanism when performing rhyme judgments, verbal recall, and task switching without an extrinsic cue (requiring them to remember what they just did). Our finding of stronger intertask correlations for participants with less inner speech is conceptually similar to Keogh et al. (2021)'s finding of stronger relationships between different visual working memory tasks in participants with aphantasia compared to those with typical visual imagery.

\hypertarget{anendophasia-a-lack-of-inner-speech}{%
\subsection{Anendophasia: A Lack of Inner Speech}\label{anendophasia-a-lack-of-inner-speech}}

People's self-reports cannot always be taken at face value (Heavey \& Hurlburt, 2008; Hurlburt, 2011; Hurlburt et al., 2013). But when people report that they rarely or never experience inner spech, they are not just confabulating. This is evident both in the consistency of their subjective responses (Roebuck \& Lupyan, 2020), and, as we report here, differences in objective performance. When investigating unusual human experiences, it helps to have a label. For example, the coining of ``aphantasia'' to the lack of visual imagery (Zeman et al., 2010) is both helpful for research -- providing a useful keyword -- and for self-identification; its introduction led to the creation of an online community with over 50,000 members (r/aphantasia).
We would therefore like to propose a name for the phenomenon of a lack of inner speech: \textbf{anendophasia}: \emph{an} (lack) + \emph{endo} (inner) + \emph{phasia} (speech). This term was developed in consultation with individuals who identify as lacking inner speech and has the benefit of including the familiar Greek root \emph{phasia} (aphasia, paraphasia, etc.). Furthermore, ``endophasia'' has precedent in being used to refer to inner speech (Bergounioux, 2001; Loevenbruck et al., 2018). The term also avoids subsuming inner speech under ``aphantasia'' (Monzel, Mitchell, Macpherson, Pearson, \& Zeman, 2022) because inner speech is both auditory and articulatory in nature (whether it is better termed ``inner hearing'' or ``inner speaking'' is debated) and because the linguistic properties of inner speech are likely not reducible to auditory and articulatory features. For these reasons, we also do not believe the previously proposed term ``anauralia'' is appropriate (Hinwar \& Lambert, 2021).

\hypertarget{relations-to-visual-imagery-auditory-imagery-and-unsymbolized-thought}{%
\subsection{Relations to Visual Imagery, Auditory Imagery and ``Unsymbolized'' Thought}\label{relations-to-visual-imagery-auditory-imagery-and-unsymbolized-thought}}

Can anendophasia be thought of simply as a lack of auditory imagery? We think not. First, many who lack inner speech report being able to engage in musical imagery (although they report ``earworms'' - intrusive musical imagery - less often than people with typical levels of inner speech). Second, although inner speech is often experienced as having phonological features -- one of the reasons people often perceive it as speech (Langland-Hassan, 2018) -- it can also involve an articulatory-motor dimension (Geva, 2018; Perrone-Bertolotti et al., 2014). The current work was not designed to investigate the separate contributions of auditory and articulatory dimensions. Paradoxically, some people also claim to experience ``wordless'' inner speech akin to a series of tip of the tongue states (Hurlburt et al., 2013).

When asked to reflect on what form their thoughts take, people who score low on both inner speech and visual imagery claim that they ``think in concepts''. What it means to ``think in concepts'' without relying on language is not clear. Beyond informal self-reports, the existence of such non-verbal and non-perceptual phenomenal experiences is supported by Descriptive Experience Sampling (DES) (Heavey \& Hurlburt, 2008; Hurlburt \& Akhter, 2006). When participants are probed at random times and asked to report on their mental states, \textasciitilde22\% of the time their reports are consistent with what Hurlburt and colleagues have called ``unsymbolized thinking''. In such episodes, people feel that they think `a particular, definite thought without awareness of that thought being conveyed as words, images, or any other symbols' (Heavey \& Hurlburt, 2008, p. 802). Unsymbolized thinking is a slippery construct that tends to be defined in terms of what it is not. For example, Hurlburt and Akhter (2008) describe it as `a thinking, not a feeling, not an intention, not an intimation, not a kinesthetic event, not a bodily event' (p.~1366). A telling example is a participant wondering if her friend will arrive in a car or pickup truck, but not experiencing any words or images; rather, the question is experienced as a single undifferentiated whole.

It is possible that such ``unsymbolized thinking'' is subserved by the same processes as inner speech, but simply lacks conscious auditory or articulatory features of inner speech (Vicente \& Martinez-Manrique, 2016). Alternatively, it may correspond to a genuinely different form of experience in which people entertain more abstract conceptual representations which are less accessible to people with higher levels of inner speech and imagery.

\hypertarget{limitations}{%
\subsection{Limitations}\label{limitations}}

One limitation of our work is its reliance on wholly subjective questions for measuring inner speech. Considering that our focus is on differences in phenomenology, this is appropriate. At the same time, there is reason to be skeptical of people's assessments of their inner experiences. People can be wrong about what they think they experience (Hurlburt \& Schwitzgebel, 2011). It would be therefore helpful to supplement subjective assessments with objective ones of the sort becoming possible for differences in visual imagery (Kay, Keogh, Andrillon, \& Pearson, 2022). Relatedly, since inner speech is known to vary not just between people, but also across situations (Fernyhough, 2004; Grandchamp et al., 2019; Oppenheim \& Dell, 2010), it is worth examining whether people who report having little inner speech experience more of it if placed in situations that, e.g., benefit from verbal rehearsal.

Another limitation is the remaining possibility that differences we ascribe to inner speech come from third factors such, e.g., a general difference in introspection and/or conscientiousness. Although we cannot rule out all such possible confounds, it is worth noting that differences in inner speech, while correlated with e.g., visual imagery, are dissociable from it. There is also no evidence that inner speech was associated with across-the-board differences in performance. Future studies could counter these potential confounds to some extent by explicitly manipulating inner speech, for example by interfering with it or instructing participants to use it in specific ways. Our effects were specific to certain task and condition combinations. We believe our results generalize across age, gender, and educational status, but it is an open question whether any of the relationships we report are specific to English speakers or Westerners.

A further limitation is that our measurement of inner speech does not distinguish between its prevalence in one's conscious experience and one's ability to control its deployment. In ongoing work we have found that people who report experiencing more inner speech (measured as in the present studies) report having a harder time shutting it off (anecdotally, one reported benefit of mindfulness meditation is precisely this ability; for someone with less inner speech to begin with, there is less to shut off). At the same time, we suspect that there is a wide range in ability to regulate one's inner speech, perhaps related to more domain-general cognitive control skills.

Lastly, while the term ``anendophasia'' connotes \emph{lack} of inner speech, many of the participants in our ``low inner speech'' group reported having \emph{some} inner speech. Screening a larger group to identify people who do not endorse having \emph{any} inner speech would help in knowing whether the cognitive consequences of having less inner speech are continuous with having none.

\hypertarget{conclusion}{%
\section{Conclusion}\label{conclusion}}

Not everyone experiences inner speech. We proposed a name for a lack of inner speech: anendophasia. People who experience less inner speech were worse at making rhyme judgments in response to images and remembering a list of words. Task switching performance was not, however, either slower or less accurate. Taken together, our experiments suggest that there are real behavioral consequences of experiencing less or more inner speech, and that these differences may often be masked because people with anendophasia use alternate strategies to achieve similar overall performance.

\newpage

\hypertarget{references}{%
\section{References}\label{references}}

\hypertarget{refs}{}
\begin{CSLReferences}{1}{0}
\leavevmode\vadjust pre{\hypertarget{ref-alderson2015inner}{}}%
Alderson-Day, B., \& Fernyhough, C. (2015). Inner speech: Development, cognitive functions, phenomenology, and neurobiology. \emph{Psychological Bulletin}, \emph{141}(5), 931--965.

\leavevmode\vadjust pre{\hypertarget{ref-alderson2018varieties}{}}%
Alderson-Day, B., Mitrenga, K., Wilkinson, S., McCarthy-Jones, S., \& Fernyhough, C. (2018). The varieties of inner speech questionnaire--revised (VISQ-r): Replicating and refining links between inner speech and psychopathology. \emph{Consciousness and Cognition}, \emph{65}, 48--58.

\leavevmode\vadjust pre{\hypertarget{ref-baddeley1966short}{}}%
Baddeley, A. (1966). Short-term memory for word sequences as a function of acoustic, semantic and formal similarity. \emph{Quarterly Journal of Experimental Psychology}, \emph{18}(4), 362--365.

\leavevmode\vadjust pre{\hypertarget{ref-baddeley2001working}{}}%
Baddeley, A., Chincotta, D., \& Adlam, A. (2001). Working memory and the control of action: Evidence from task switching. \emph{Journal of Experimental Psychology: General}, \emph{130}(4), 641.

\leavevmode\vadjust pre{\hypertarget{ref-bergounioux2001}{}}%
Bergounioux, G. (2001). Endophasie et linguistique {[}décomptes, quotes et squelette{]}. \emph{Langue Francaise}, \emph{132}, 106--124.

\leavevmode\vadjust pre{\hypertarget{ref-bermudez2007thinking}{}}%
Bermúdez, J. L. (2007). \emph{Thinking without words}. Oxford University Press.

\leavevmode\vadjust pre{\hypertarget{ref-brinthaupt2019individual}{}}%
Brinthaupt, T. M. (2019). Individual differences in self-talk frequency: Social isolation and cognitive disruption. \emph{Frontiers in Psychology}, \emph{10}, 1088.

\leavevmode\vadjust pre{\hypertarget{ref-brinthaupt2009self}{}}%
Brinthaupt, T. M., Hein, M. B., \& Kramer, T. E. (2009). The self-talk scale: Development, factor analysis, and validation. \emph{Journal of Personality Assessment}, \emph{91}(1), 82--92.

\leavevmode\vadjust pre{\hypertarget{ref-carruthers2002cognitive}{}}%
Carruthers, P. (2002). The cognitive functions of language. \emph{Behavioral and Brain Sciences}, \emph{25}(6), 657--674.

\leavevmode\vadjust pre{\hypertarget{ref-chella2020cognitive}{}}%
Chella, A., \& Pipitone, A. (2020). A cognitive architecture for inner speech. \emph{Cognitive Systems Research}, \emph{59}, 287--292.

\leavevmode\vadjust pre{\hypertarget{ref-clark_magicwords}{}}%
Clark, A. (1998). \emph{Language and thought: Interdisciplinary themes} (P. Carruthers \& J. Boucher, Eds.). Cambridge University Press.

\leavevmode\vadjust pre{\hypertarget{ref-cragg2010language}{}}%
Cragg, L., \& Nation, K. (2010). Language and the development of cognitive control. \emph{Topics in Cognitive Science}, \emph{2}(4), 631--642.

\leavevmode\vadjust pre{\hypertarget{ref-dawes2020cognitive}{}}%
Dawes, A. J., Keogh, R., Andrillon, T., \& Pearson, J. (2020). A cognitive profile of multi-sensory imagery, memory and dreaming in aphantasia. \emph{Scientific Reports}, \emph{10}(1), 1--10.

\leavevmode\vadjust pre{\hypertarget{ref-de2015jspsych}{}}%
De Leeuw, J. R. (2015). jsPsych: A JavaScript library for creating behavioral experiments in a web browser. \emph{Behavior Research Methods}, \emph{47}(1), 1--12.

\leavevmode\vadjust pre{\hypertarget{ref-dunabeitia2018multipic}{}}%
Duñabeitia, J. A., Crepaldi, D., Meyer, A. S., New, B., Pliatsikas, C., Smolka, E., \& Brysbaert, M. (2018). MultiPic: A standardized set of 750 drawings with norms for six european languages. \emph{Quarterly Journal of Experimental Psychology}, \emph{71}(4), 808--816.

\leavevmode\vadjust pre{\hypertarget{ref-emerson2003role}{}}%
Emerson, M. J., \& Miyake, A. (2003). The role of inner speech in task switching: A dual-task investigation. \emph{Journal of Memory and Language}, \emph{48}(1), 148--168.

\leavevmode\vadjust pre{\hypertarget{ref-felton_2020}{}}%
Felton, J. (2020). People with no internal monologue explain what it's like in their head. IFLScience. Retrieved from \url{https://www.iflscience.com/people-with-no-internal-monologue-explain-what-its-like-in-their-head-57739}

\leavevmode\vadjust pre{\hypertarget{ref-fernyhough_alien_2004}{}}%
Fernyhough, C. (2004). Alien voices and inner dialogue: Towards a developmental account of auditory verbal hallucinations. \emph{New Ideas in Psychology}, \emph{22}(1), 49--68. \url{https://doi.org/10.1016/j.newideapsych.2004.09.001}

\leavevmode\vadjust pre{\hypertarget{ref-fernyhough2023inner}{}}%
Fernyhough, C., \& Borghi, A. M. (2023). Inner speech as language process and cognitive tool. \emph{Trends in Cognitive Sciences}.

\leavevmode\vadjust pre{\hypertarget{ref-forder2019hearing}{}}%
Forder, L., \& Lupyan, G. (2019). Hearing words changes color perception: Facilitation of color discrimination by verbal and visual cues. \emph{Journal of Experimental Psychology: General}, \emph{148}(7), 1105--1123.

\leavevmode\vadjust pre{\hypertarget{ref-frankish_isnv}{}}%
Frankish, K. (2018). \emph{Inner speech: New voices} (P. Langland-Hassan \& A. Vicente, Eds.). Oxford University Press.

\leavevmode\vadjust pre{\hypertarget{ref-gauker2011words}{}}%
Gauker, C. (2011). \emph{Words and images: An essay on the origin of ideas}. Oxford University Press, Oxford.

\leavevmode\vadjust pre{\hypertarget{ref-geva_isnv}{}}%
Geva, S. (2018). \emph{Inner speech: New voices} (P. Langland-Hassan \& A. Vicente, Eds.). Oxford University Press.

\leavevmode\vadjust pre{\hypertarget{ref-geva2011discrepancy}{}}%
Geva, S., Bennett, S., Warburton, E. A., \& Patterson, K. (2011). Discrepancy between inner and overt speech: Implications for post-stroke aphasia and normal language processing. \emph{Aphasiology}, \emph{25}(3), 323--343.

\leavevmode\vadjust pre{\hypertarget{ref-geva2019test}{}}%
Geva, S., \& Warburton, E. A. (2019). A test battery for inner speech functions. \emph{Archives of Clinical Neuropsychology}, \emph{34}(1), 97--113.

\leavevmode\vadjust pre{\hypertarget{ref-grandchamp_condialint_2019}{}}%
Grandchamp, R., Rapin, L., Perrone-Bertolotti, M., Pichat, C., Haldin, C., Cousin, E., \ldots{} Lœvenbruck, H. (2019). The {ConDialInt} {Model}: {Condensation}, {Dialogality}, and {Intentionality} {Dimensions} of {Inner} {Speech} {Within} a {Hierarchical} {Predictive} {Control} {Framework}. \emph{Frontiers in Psychology}, \emph{10}, 2019. \url{https://doi.org/10.3389/fpsyg.2019.02019}

\leavevmode\vadjust pre{\hypertarget{ref-gregory2016inner}{}}%
Gregory, D. (2016). Inner speech, imagined speech, and auditory verbal hallucinations. \emph{Review of Philosophy and Psychology}, \emph{7}, 653--673.

\leavevmode\vadjust pre{\hypertarget{ref-heavey2008phenomena}{}}%
Heavey, C. L., \& Hurlburt, R. T. (2008). The phenomena of inner experience. \emph{Consciousness and Cognition}, \emph{17}(3), 798--810.

\leavevmode\vadjust pre{\hypertarget{ref-Hinwar2021}{}}%
Hinwar, R. P., \& Lambert, A. J. (2021). Anauralia: The silent mind and its association with aphantasia. \emph{Frontiers in Psychology}, \emph{12}. \url{https://doi.org/10.3389/fpsyg.2021.744213}

\leavevmode\vadjust pre{\hypertarget{ref-Hurlburt2011}{}}%
Hurlburt, R. T. (2011). \emph{Investigating pristine inner experience}. Cambridge University Press. \url{https://doi.org/10.1017/cbo9780511842627}

\leavevmode\vadjust pre{\hypertarget{ref-hurlburt2006descriptive}{}}%
Hurlburt, R. T., \& Akhter, S. A. (2006). The descriptive experience sampling method. \emph{Phenomenology and the Cognitive Sciences}, \emph{5}(3), 271--301.

\leavevmode\vadjust pre{\hypertarget{ref-hurlburt2008unsymbolized}{}}%
Hurlburt, R. T., \& Akhter, S. A. (2008). Unsymbolized thinking. \emph{Consciousness and Cognition}, \emph{17}(4), 1364--1374.

\leavevmode\vadjust pre{\hypertarget{ref-Hurlburt2013}{}}%
Hurlburt, R. T., Heavey, C. L., \& Kelsey, J. M. (2013). Toward a phenomenology of inner speaking. \emph{Consciousness and Cognition}, \emph{22}(4), 1477--1494. \url{https://doi.org/10.1016/j.concog.2013.10.003}

\leavevmode\vadjust pre{\hypertarget{ref-hurlburt2011describing}{}}%
Hurlburt, R. T., \& Schwitzgebel, E. (2011). \emph{Describing inner experience?: Proponent meets skeptic}. Mit Press.

\leavevmode\vadjust pre{\hypertarget{ref-kay2022pupillary}{}}%
Kay, L., Keogh, R., Andrillon, T., \& Pearson, J. (2022). The pupillary light response as a physiological index of aphantasia, sensory and phenomenological imagery strength. \emph{Elife}, \emph{11}, e72484.

\leavevmode\vadjust pre{\hypertarget{ref-keogh2021visual}{}}%
Keogh, R., Wicken, M., \& Pearson, J. (2021). Visual working memory in aphantasia: Retained accuracy and capacity with a different strategy. \emph{Cortex}, \emph{143}, 237--253.

\leavevmode\vadjust pre{\hypertarget{ref-langlandhassan_isnv}{}}%
Langland-Hassan, P. (2018). \emph{Inner speech: New voices} (P. Langland-Hassan \& A. Vicente, Eds.). Oxford University Press.

\leavevmode\vadjust pre{\hypertarget{ref-langland2015inner}{}}%
Langland-Hassan, P., Faries, F. R., Richardson, M. J., \& Dietz, A. (2015). Inner speech deficits in people with aphasia. \emph{Frontiers in Psychology}, \emph{6}, 528.

\leavevmode\vadjust pre{\hypertarget{ref-laurent2016inner}{}}%
Laurent, L., Millot, J.-L., Andrieu, P., Camos, V., Floccia, C., \& Mathy, F. (2016). Inner speech sustains predictable task switching: Direct evidence in adults. \emph{Journal of Cognitive Psychology}, \emph{28}(5), 585--592.

\leavevmode\vadjust pre{\hypertarget{ref-LIDSTONE2010438}{}}%
Lidstone, J. S. M., Meins, E., \& Fernyhough, C. (2010). The roles of private speech and inner speech in planning during middle childhood: Evidence from a dual task paradigm. \emph{Journal of Experimental Child Psychology}, \emph{107}(4), 438--451. https://doi.org/\url{https://doi.org/10.1016/j.jecp.2010.06.002}

\leavevmode\vadjust pre{\hypertarget{ref-loevenbruck_isnv}{}}%
Loevenbruck, H., Grandchamp, R., Rapin, L., Nalborczyk, L., Dohen, M., Perrier, P., \ldots{} Perrone-Bertolotti, M. (2018). \emph{Inner speech: New voices} (P. Langland-Hassan \& A. Vicente, Eds.). Oxford University Press.

\leavevmode\vadjust pre{\hypertarget{ref-lupyan2010conceptual}{}}%
Lupyan, G., Thompson-Schill, S. L., \& Swingley, D. (2010). Conceptual penetration of visual processing. \emph{Psychological Science}, \emph{21}(5), 682--691.

\leavevmode\vadjust pre{\hypertarget{ref-lupyan2023hidden}{}}%
Lupyan, G., Uchiyama, R., Thompson, B., \& Casasanto, D. (2023). Hidden differences in phenomenal experience. \emph{Cognitive Science}, \emph{47}(1), e13239.

\leavevmode\vadjust pre{\hypertarget{ref-mccarthy2011varieties}{}}%
McCarthy-Jones, S., \& Fernyhough, C. (2011). The varieties of inner speech: Links between quality of inner speech and psychopathological variables in a sample of young adults. \emph{Consciousness and Cognition}, \emph{20}(4), 1586--1593.

\leavevmode\vadjust pre{\hypertarget{ref-miyake2004inner}{}}%
Miyake, A., Emerson, M. J., Padilla, F., \& Ahn, J. (2004). Inner speech as a retrieval aid for task goals: The effects of cue type and articulatory suppression in the random task cuing paradigm. \emph{Acta Psychologica}, \emph{115}(2-3), 123--142.

\leavevmode\vadjust pre{\hypertarget{ref-Monzel2022}{}}%
Monzel, M., Mitchell, D., Macpherson, F., Pearson, J., \& Zeman, A. (2022). Aphantasia, dysikonesia, anauralia: Call for a single term for the lack of mental imagery{\textendash}commentary on dance et~al. (2021) and hinwar and lambert (2021). \emph{Cortex}, \emph{150}, 149--152. \url{https://doi.org/10.1016/j.cortex.2022.02.002}

\leavevmode\vadjust pre{\hypertarget{ref-morin_isnv}{}}%
Morin, A. (2018). \emph{Inner speech: New voices} (P. Langland-Hassan \& A. Vicente, Eds.). Oxford University Press.

\leavevmode\vadjust pre{\hypertarget{ref-morin2018self}{}}%
Morin, A., Duhnych, C., \& Racy, F. (2018). Self-reported inner speech use in university students. \emph{Applied Cognitive Psychology}, \emph{32}(3), 376--382.

\leavevmode\vadjust pre{\hypertarget{ref-murray1968articulation}{}}%
Murray, D. (1968). Articulation and acoustic confusability in short-term memory. \emph{Journal of Experimental Psychology}, \emph{78}(4p1), 679--684.

\leavevmode\vadjust pre{\hypertarget{ref-nalborczyk2023distinct}{}}%
Nalborczyk, L., Longcamp, M., Bonnard, M., Serveau, V., Spieser, L., \& Alario, F.-X. (2023). Distinct neural mechanisms support inner speaking and inner hearing. \emph{Cortex}, \emph{169}, 161--173.

\leavevmode\vadjust pre{\hypertarget{ref-nedergaard2023mind}{}}%
Nedergaard, J. S. K., Christensen, M. S., \& Wallentin, M. (2023). Mind over body: Interfering with the inner voice is detrimental to endurance performance. \emph{Psychology of Sport and Exercise}, 102472.

\leavevmode\vadjust pre{\hypertarget{ref-nedergaard2022verbal}{}}%
Nedergaard, J. S. K., Wallentin, M., \& Lupyan, G. (2022). Verbal interference paradigms: A systematic review investigating the role of language in cognition. \emph{Psychonomic Bulletin \& Review}, 1--25.

\leavevmode\vadjust pre{\hypertarget{ref-oppenheim2010motor}{}}%
Oppenheim, G. M., \& Dell, G. S. (2010). Motor movement matters: The flexible abstractness of inner speech. \emph{Memory \& Cognition}, \emph{38}(8), 1147--1160.

\leavevmode\vadjust pre{\hypertarget{ref-perrone2014little}{}}%
Perrone-Bertolotti, M., Rapin, L., Lachaux, J.-P., Baciu, M., \& Loevenbruck, H. (2014). What is that little voice inside my head? Inner speech phenomenology, its role in cognitive performance, and its relation to self-monitoring. \emph{Behavioural Brain Research}, \emph{261}, 220--239.

\leavevmode\vadjust pre{\hypertarget{ref-perry2014role}{}}%
Perry, L. K., \& Lupyan, G. (2014). The role of language in multi-dimensional categorization: Evidence from transcranial direct current stimulation and exposure to verbal labels. \emph{Brain and Language}, \emph{135}, 66--72.

\leavevmode\vadjust pre{\hypertarget{ref-R-base}{}}%
R Core Team. (2022). \emph{R: A language and environment for statistical computing}. Vienna, Austria: R Foundation for Statistical Computing. Retrieved from \url{https://www.R-project.org/}

\leavevmode\vadjust pre{\hypertarget{ref-racy2020using}{}}%
Racy, F., Morin, A., \& Duhnych, C. (2020). Using a thought listing procedure to construct the general inner speech questionnaire: An ecological approach. \emph{Journal of Constructivist Psychology}, \emph{33}(4), 385--405.

\leavevmode\vadjust pre{\hypertarget{ref-roebuck2020internal}{}}%
Roebuck, H., \& Lupyan, G. (2020). The internal representations questionnaire: Measuring modes of thinking. \emph{Behavior Research Methods}, \emph{52}(5), 2053--2070.

\leavevmode\vadjust pre{\hypertarget{ref-rossion2004revisiting}{}}%
Rossion, B., \& Pourtois, G. (2004). Revisiting snodgrass and vanderwart's object pictorial set: The role of surface detail in basic-level object recognition. \emph{Perception}, \emph{33}(2), 217--236.

\leavevmode\vadjust pre{\hypertarget{ref-soloducha_2020}{}}%
Soloducha, A. (2020). What it's like living without an inner monologue. CBC News. Retrieved from \url{https://www.cbc.ca/news/canada/saskatchewan/inner-monologue-experience-science-1.5486969}

\leavevmode\vadjust pre{\hypertarget{ref-tian2016mental}{}}%
Tian, X., Zarate, J. M., \& Poeppel, D. (2016). Mental imagery of speech implicates two mechanisms of perceptual reactivation. \emph{Cortex}, \emph{77}, 1--12.

\leavevmode\vadjust pre{\hypertarget{ref-vicente2016nature}{}}%
Vicente, A., \& Martinez-Manrique, F. (2016). The nature of unsymbolized thinking. \emph{Philosophical Explorations}, \emph{19}(2), 173--187.

\leavevmode\vadjust pre{\hypertarget{ref-wallace2017interfering}{}}%
Wallace, G. L., Peng, C. S., \& Williams, D. (2017). Interfering with inner speech selectively disrupts problem solving and is linked with real-world executive functioning. \emph{Journal of Speech, Language, and Hearing Research}, \emph{60}(12), 3456--3460.

\leavevmode\vadjust pre{\hypertarget{ref-winawer2007russian}{}}%
Winawer, J., Witthoft, N., Frank, M. C., Wu, L., Wade, A. R., \& Boroditsky, L. (2007). Russian blues reveal effects of language on color discrimination. \emph{Proceedings of the National Academy of Sciences}, \emph{104}(19), 7780--7785.

\leavevmode\vadjust pre{\hypertarget{ref-zeman2010}{}}%
Zeman, A. Z., Della Sala, S., Torrens, L. A., Gountouna, V. E., McGonigle, D. J., \& Logie, R. H. (2010). Loss of imagery phenomenology with intact visuo-spatial task performance: A case of 'blind imagination'. \emph{Neuropsychologia}, \emph{48}, 145--155.

\end{CSLReferences}


\end{document}
