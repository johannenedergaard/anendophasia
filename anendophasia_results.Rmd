---
title: "Anendophasia analysis"
output:
  html_document:
    df_print: paged
    keep_md: true
---

Excluded 10 participants for responding randomly, missing at least one out of the four experiments, or otherwise not complying with task instructions. This leaves us with 47 high verbal and 46 low verbal. All the plots visualize categorical differences between the two groups while all the statistical models use verbal score as a continuous predictor.

```{r, include=FALSE}
# read in full browser interaction data
library(tidyverse)
library(lme4)
library(lmerTest)
library(kableExtra)
library(optimx)
library(ggpubr)
library(forcats)
library(corrplot)
library(cowplot)
library(ggforce)
#library(Rmisc)
source("summarySEwithin_imp.R")
color_palette <- c('#88CCEE', '#44AA99', '#117733', '#332288', '#DDCC77', '#999933','#CC6677', '#882255', '#AA4499', '#DDDDDD') # Tol_muted from https://zenodo.org/record/3381072#.Y0_5ilJBw-Q

browser_interactions <- read.csv('browser_df_anendophasia_full.csv',row.names = 1)
irq_scores <- read.csv('irq_scores.csv', row.names = 1)
```

## Same/different judgments

Prior to this excluded trials above 5 seconds and below 200 ms.

```{r, include=FALSE}
same_different_trials <- read.csv('same_different_trials_221028.csv',row.names = 1)
```

#### Descriptive statistics by group: Same/different judgments

```{r, include=FALSE}
# how many correct
100-(table(same_different_trials$correct)[1]/table(same_different_trials$correct)[2]*100)
# how many correct by group
SD_correct <- same_different_trials %>%
  dplyr::group_by(high_low_verbal) %>%
  dplyr::summarise(correct = sum(correct)/n())
SD_correct

SD_RT <- same_different_trials %>%
  dplyr::group_by(high_low_verbal) %>%
  dplyr::summarise(rt = mean(rt,na.rm=T))
SD_RT
```

Generally, participants made the correct judgment on `r round(100-(table(same_different_trials$correct)[1]/table(same_different_trials$correct)[2]*100), 2)` % of trials. This did not differ between the high verbal (`r round(SD_correct$correct[1]*100,2)` %) and the low verbal group (`r round(SD_correct$correct[2]*100,2)`. In subsequent analyses and plots, we only include correct trials. See Figure XX below for reaction times between the high verbal and low verbal groups for category ('do these two animals belong to the same category?') or identity ('are these two animals identical?') judgments.

```{r, include=FALSE}
SD_rt_df <- same_different_trials %>%
  filter(correct == 1) %>%
  summarySEwithin2(measurevar = 'rt', betweenvars = c('high_low_verbal'),
                  withinvars = 'judgment_type',idvar = 'worker_id')
SD_rt_df_individual <- same_different_trials %>%
  filter(correct == 1) %>%
  summarySEwithin2(measurevar = 'rt', betweenvars = c('high_low_verbal', 'worker_id'),
                  withinvars = 'judgment_type',idvar = 'worker_id')
```



```{r, echo=FALSE}
pd <- position_dodge(width = 0.2)
ggplot(SD_rt_df, aes(judgment_type, rt, color=high_low_verbal)) +
  geom_sina(data= SD_rt_df_individual, aes(judgment_type, rt), alpha=0.3)+
  geom_errorbar(aes(ymin=rt-ci, ymax=rt+ci), width=.1, position= pd) +
  stat_summary(fun = mean, geom = 'point', aes(group = high_low_verbal), position=pd) +
  stat_summary(fun = mean, geom = 'line', aes(group = high_low_verbal), size = 1,position=pd)+
  theme_bw() +
  labs(y ='RT', title = 'Judgment types and verbal score', x= 'Category or identity judgments')+
  scale_color_manual(values = color_palette[c(4,6)])
```

#### Statistical models: Same/different judgments

```{r, include=FALSE, cache = TRUE}
category_model <- lmer(log(rt) ~ judgment_type + VerbalScored + (1|worker_id), 
                        subset(same_different_trials, correct ==1))
summary(category_model)
```

We conducted a linear mixed model of verbal score and judgment type predicting log-transformed reaction time including random intercepts per participant. This model indicated  significant main effect of judgment type and a marginally significant effect of verbal score. Identity judgments were faster than category judgments ($\beta$ = `r round(summary(category_model)$coefficients[2,1],2)`, SE = `r round(summary(category_model)$coefficients[2,2],2)`, t = `r round(summary(category_model)$coefficients[2,4],2)`, p < .001), and a higher verbal score was marginally associated with faster reaction times ($\beta$ = `r round(summary(category_model)$coefficients[3,1],2)`, SE = `r round(summary(category_model)$coefficients[3,2],2)`, t = `r round(summary(category_model)$coefficients[3,4],2)`, p = `r round(summary(category_model)$coefficients[3,5],3)`).

The key test for this experiment was whether the two groups behaved differently when giving correct 'DIFFERENT' responses on identity trials when the two images belonged to the same category. That is, we expected high verbal participants to be more susceptible to interference from a same-category distractor.

```{r, include=F}
SD_rt_df_key_comparison <- same_different_trials %>%
  filter(judgment_type == 'identical_image' & correct ==1 & answer =='different') %>%
  summarySEwithin2(measurevar = 'rt', betweenvars = c('high_low_verbal'),
                  withinvars = c('judgment_type', 'same_category_animal'),idvar = 'worker_id')

SD_rt_df_key_comparison_individual <- same_different_trials %>%
  filter(judgment_type == 'identical_image' & correct ==1 & answer =='different') %>%
  summarySEwithin2(measurevar = 'rt', betweenvars = c('high_low_verbal', 'worker_id'),
                  withinvars = c('judgment_type', 'same_category_animal'),idvar = 'worker_id')
```

```{r}
ggplot(SD_rt_df_key_comparison, aes(same_category_animal, rt, color=high_low_verbal)) +
  geom_sina(data= SD_rt_df_key_comparison_individual, aes(same_category_animal, rt), alpha=0.3)+
  geom_errorbar(aes(ymin=rt-ci, ymax=rt+ci), width=.1, position= pd) +
  stat_summary(fun = mean, geom = 'point', aes(group = high_low_verbal), position= pd) +
  stat_summary(fun = mean, geom = 'line', aes(group = high_low_verbal), size = 1, position= pd)+
  theme_bw() +
  labs(y ='RT', title = 'Latency to correct DIFFERENT response on identity trials', x = 'Between or within category distractor') +
  scale_color_manual(values = color_palette[c(4,6)])
```

```{r, include=FALSE, cache = TRUE}

latency_to_different <- lmer(log(rt) ~ VerbalScored * same_category_animal + (1|worker_id),
                            subset(same_different_trials, judgment_type == 'identical_image' & correct ==1 & answer =='different'))
summary(latency_to_different)
```
A linear mixed model of log-transformed reaction time with verbal score and category membership of the distractor as predictors, including random intercepts per participant, provided evidence that high verbal participants were not particularly affected by the within-category interference (interaction effect: p = `r round(summary(latency_to_different)$coefficients[4,5],3)`). However, there was a significant main effect of category membership of the distractor with within-category distractors being associated with slower reaction times ($\beta$ = `r round(summary(latency_to_different)$coefficients[3,1],2)`, SE = `r round(summary(latency_to_different)$coefficients[3,2],2)`, t = `r round(summary(latency_to_different)$coefficients[3,4],2)`, p = `r round(summary(latency_to_different)$coefficients[3,5],3)`). 

#### Additional analyses: Same/different judgments
```{r, include= FALSE}
SD_rt_df_catdog <- same_different_trials %>%
  filter(judgment_type == 'identical_image' & correct ==1 & answer =='different') %>%
  summarySEwithin2(measurevar = 'rt', betweenvars = c('high_low_verbal'),
                  withinvars = c('judgment_type', 'same_category_animal', 'cat_or_dog'),idvar = 'worker_id')

SD_rt_df_catdog_individual <- same_different_trials %>%
  filter(judgment_type == 'identical_image' & correct ==1 & answer =='different') %>%
  summarySEwithin2(measurevar = 'rt', betweenvars = c('high_low_verbal', 'worker_id'),
                  withinvars = c('judgment_type', 'same_category_animal','cat_or_dog'),idvar = 'worker_id')
```

We also checked whether the kind of animal made a difference on a within-category distractor trial.

```{r, echo=FALSE}
ggplot(subset(SD_rt_df_catdog, cat_or_dog != 'cat-dog'), aes(cat_or_dog, rt, color=high_low_verbal)) +
geom_sina(data= subset(SD_rt_df_catdog_individual, cat_or_dog != 'cat-dog'), aes(cat_or_dog, rt), alpha=0.3)+
  geom_errorbar(aes(ymin=rt-ci, ymax=rt+ci), width=.1, position= pd) +  
  stat_summary(fun = mean, geom = 'point', aes(group = high_low_verbal),position= pd) +
  stat_summary(fun = mean, geom = 'line', aes(group = high_low_verbal), size = 1, position= pd)+
  theme_bw() +
  labs(y ='RT', title = 'Latency to correct DIFFERENT response on identity trials', x = 'Image pair') +
  scale_color_manual(values = color_palette[c(4,6)])
```
```{r include=FALSE, cache = TRUE}
kind_of_animal_model <- lmer(log(rt) ~ cat_or_dog * VerbalScored + (1|worker_id), 
                              data = subset(same_different_trials, judgment_type =='identical_image' & correct & answer=='same'))
summary(kind_of_animal_model) 
```
A linear mixed model of log-transformed reaction times with verbal score and animal pair (dog-dog or cat-cat) as predictors, including random intercepts per participant, provided evidence that dog-dog trials were faster than cat-cat trials ($\beta$ = `r round(summary(kind_of_animal_model)$coefficients[2,1],2)`, SE = `r round(summary(kind_of_animal_model)$coefficients[2,2],2)`, t = `r round(summary(kind_of_animal_model)$coefficients[2,4],2)`, p < .001). The model corroborated the result that a higher verbal score was associated with faster reaction times ($\beta$ = `r round(summary(kind_of_animal_model)$coefficients[3,1],2)`, SE = `r round(summary(kind_of_animal_model)$coefficients[3,2],2)`, t = `r round(summary(kind_of_animal_model)$coefficients[3,4],2)`, p = `r round(summary(kind_of_animal_model)$coefficients[3,5],3)`). However, this effect of verbal score was less strong when the stimuli were dog-dog than when they were cat-cat as indicated by a significant interaction effect between verbal score and animal pair ($\beta$ = `r round(summary(kind_of_animal_model)$coefficients[4,1],2)`, SE = `r round(summary(kind_of_animal_model)$coefficients[4,2],2)`, t = `r round(summary(kind_of_animal_model)$coefficients[4,4],2)`, p = `r round(summary(kind_of_animal_model)$coefficients[4,5],3)`).

#### Strategies: Same/different judgments

In this experiment, most participants said that they had no particular strategy. However, eight of the high-verbal participants and one of the low-verbal participants explicitly mentioned something to do with verbalizing the problems (e.g. 'In my head I said "same" or "different" before I pressed the arrow key.')

## Rhyme judgments

Excluded five rhyming pairs as they had below-chance performance for at least one group. These were bin/chin, cab/crab, rake/cake, wave/cave, and park/shark. 

```{r, include =FALSE}
rhyming_trials <- read.csv('rhyming_trials_221028.csv',row.names = 1)
```

```{r, include=FALSE}
rhyme_desc_df_rt <- rhyming_trials %>%
  summarySEwithin2(measurevar = c('rt'), betweenvars = 'high_low_verbal',
                  withinvars = 'type', idvar='worker_id', na.rm=T)
rhyme_desc_df_correct <- rhyming_trials %>%
  summarySEwithin2(measurevar = 'correct', betweenvars = 'high_low_verbal',
                  withinvars = 'type', idvar='worker_id')
rhyme_desc_df <- cbind(rhyme_desc_df_rt, rhyme_desc_df_correct)
colnames(rhyme_desc_df) <- make.names(colnames(rhyme_desc_df),unique = T)
colnames(rhyme_desc_df)[8] <- 'ci_rt'
colnames(rhyme_desc_df)[16] <- 'ci_accuracy'
rhyme_desc_df <- rhyme_desc_df %>% select(high_low_verbal, type, rt,ci_rt, correct,ci_accuracy) 

rhyme_rt_df <- rhyming_trials %>%
  filter(correct == 1) %>%
  summarySEwithin2(measurevar = 'rt', betweenvars = c('high_low_verbal','talk_out_loud'),
                  withinvars = c('type'),idvar = 'worker_id',na.rm=T)
rhyme_rt_df_individual <- rhyming_trials %>%
  filter(correct == 1) %>%
  summarySEwithin2(measurevar = 'rt', betweenvars = c('high_low_verbal', 'worker_id', 'talk_out_loud'),
                  withinvars = c('type'),idvar = 'worker_id', na.rm = T)
rhyme_correct_df <- rhyming_trials %>%
  summarySEwithin2(measurevar = 'correct', betweenvars = c('high_low_verbal', 'talk_out_loud'), withinvars = c('type'),idvar = 'worker_id')
rhyme_correct_df_individual <- rhyming_trials %>%
  summarySEwithin2(measurevar = 'correct', betweenvars = c('high_low_verbal', 'worker_id', 'talk_out_loud'),  withinvars = c('type'),idvar = 'worker_id')
```
#### Descriptive statistics by group: Rhyme judgments

Here is a table of accuracy and reaction time for the two groups (high and low verbal) across types of rhyming trials.

```{r, echo=FALSE}
rhyme_desc_df %>%
  dplyr::mutate(correct = correct * 100, ci_accuracy = ci_accuracy*100) %>%
  kable(digits=2) %>%
  kable_styling(bootstrap_options = "striped")
```
As can be seen in this table, high verbal participants were generally both faster and more accurate than low verbal participants on all three types of trials. See also figures below.

```{r, echo=FALSE}
ggplot(rhyme_desc_df_rt, aes(type, rt, color=high_low_verbal)) +
  geom_sina(data= rhyme_rt_df_individual, aes(type, rt), alpha=0.3)+
  geom_errorbar(aes(ymin=rt-ci, ymax=rt+ci), width=.1, position= pd) +  
  stat_summary(fun = mean, geom = 'point', aes(group = high_low_verbal), position= pd) +
  stat_summary(fun = mean, geom = 'line', aes(group = high_low_verbal), size = 1, position= pd)+
  theme_bw() +
  labs(y ='Reaction time', title = 'Rhyming')+
  scale_color_manual(values = color_palette[c(4,6)])
```
```{r, echo=FALSE}
ggplot(rhyme_desc_df_correct, aes(type, correct, color=high_low_verbal)) +
  geom_sina(data= rhyme_correct_df_individual, aes(type, correct), alpha=0.3)+
  geom_errorbar(aes(ymin=correct-ci, ymax=correct+ci), width=.1, position= pd) + 
  stat_summary(fun = mean, geom = 'point', aes(group = high_low_verbal),  position= pd) +
  stat_summary(fun = mean, geom = 'line', aes(group = high_low_verbal), size = 1,  position= pd)+
  theme_bw() +
  labs(y ='Accuracy', title = 'Rhyming')+
  scale_color_manual(values = color_palette[c(4,6)])
```

#### Statistical models: Rhyme judgments


```{r, include=FALSE, cache = TRUE}

rhyme_rt_m <- lmer(log(rt) ~ type * VerbalScored + name_agreement_img1 + (1|worker_id),
                    subset(rhyming_trials, correct ==1))
summary(rhyme_rt_m) 
rhyme_acc_m <- glmer(correct ~ type * VerbalScored + name_agreement_img1 + (1|worker_id),
                     family='binomial',
                     rhyming_trials,
                     control = glmerControl(optimizer ='bobyqa', optCtrl=list(maxfun=2e5)))
summary(rhyme_acc_m) 

```

A model of verbal score, rhyme type, and name agreement for the first image predicting log-transformed reaction time showed no main effect of verbal score ($\beta$ = `r round(summary(rhyme_rt_m)$coefficients[4,1],2)`, SE = `r round(summary(rhyme_rt_m)$coefficients[4,2], 2)`, t = `r round(summary(rhyme_rt_m)$coefficients[4,4],2)`, p = `r round(summary(rhyme_rt_m)$coefficients[4,5],3)`), but it did find a marginally significant effect of no-rhyme type being slower than non-orthographic rhyme ($\beta$ = `r round(summary(rhyme_rt_m)$coefficients[2,1],2)`, SE = `r round(summary(rhyme_rt_m)$coefficients[2,2], 2)`, t = `r round(summary(rhyme_rt_m)$coefficients[2,4],2)`, p = `r round(summary(rhyme_rt_m)$coefficients[2,5],3)`) and a significant effect of name agremeent being associated with faster reaction times ($\beta$ = `r round(summary(rhyme_rt_m)$coefficients[5,1],2)`, SE = `r round(summary(rhyme_rt_m)$coefficients[5,2], 2)`, t = `r round(summary(rhyme_rt_m)$coefficients[5,4],2)`, p < .001).  There were no significant interactions between rhyme type and verbal score. Another model of verbal score, rhyme type, and name agreement for the first image predicting accuracy showed that no-rhyme trials were easier than non-orthographic trials ($\beta$ = `r round(summary(rhyme_acc_m)$coefficients[2,1],2)`, SE = `r round(summary(rhyme_acc_m)$coefficients[2,2], 2)`, z = `r round(summary(rhyme_acc_m)$coefficients[2,3],2)`, p = `r round(summary(rhyme_acc_m)$coefficients[2,4],3)`) and that a higher verbal score was associated with a higher likelihood of responding accurately ($\beta$ = `r round(summary(rhyme_acc_m)$coefficients[4,1],2)`, SE = `r round(summary(rhyme_acc_m)$coefficients[4,2], 2)`, z = `r round(summary(rhyme_acc_m)$coefficients[4,3],2)`, p = `r round(summary(rhyme_acc_m)$coefficients[4,4],3)`). It also showed that trials with images with higher name agreement were significantly easier ($\beta$ = `r round(summary(rhyme_acc_m)$coefficients[5,1],2)`, SE = `r round(summary(rhyme_acc_m)$coefficients[5,2], 2)`, z = `r round(summary(rhyme_acc_m)$coefficients[5,3],2)`, p < .001). There were no significant interactions between rhyme type and verbal score.

#### Strategies: Rhyme judgments

```{r, include=F}
talk_out_loud_rhyme <- rhyming_trials %>%
  group_by(worker_id, talk_out_loud, high_low_verbal) %>%
  tally()
table(talk_out_loud_rhyme$talk_out_loud)
tol_rhyme <-talk_out_loud_rhyme %>%
  group_by(high_low_verbal,talk_out_loud)
table(tol_rhyme$high_low_verbal, tol_rhyme$talk_out_loud)
test <- chisq.test(tol_rhyme$high_low_verbal, tol_rhyme$talk_out_loud)
```

We were interested in whether participants said the words out loud to make the rhyme judgments and so we included this as a question at the end of the rhyming experiment. A chi-squared test showed that there was no significant difference between how many high-verbal participants (23 out of 47) and how many low-verbal participants (21 out of 46) reported that they had said the words out loud ($\chi^2$(1) = `r round(chisq.test(tol_rhyme$high_low_verbal, tol_rhyme$talk_out_loud)$statistic, 2)`, p = `r round(chisq.test(tol_rhyme$high_low_verbal, tol_rhyme$talk_out_loud)$p.value,3)`). Nevertheless, the effect of doing so was interestingly different for the two groups as can be seen in the figure below.

```{r, echo=F}
# does it matter what strategy they used?
ggplot(rhyme_correct_df, aes(talk_out_loud, correct, color=high_low_verbal)) +
  geom_sina(data= rhyme_correct_df_individual, aes(talk_out_loud, correct), alpha=0.3)+
  geom_errorbar(aes(ymin=correct-ci, ymax=correct+ci), width=.1, position= pd) +  
  stat_summary(fun = mean, geom = 'point', aes(group = high_low_verbal), position= pd) +
  stat_summary(fun = mean, geom = 'line', aes(group = high_low_verbal), size = 1, position= pd)+
  theme_bw() +
  labs(y ='Accuracy', title = 'Rhyming x talking out loud')+
  scale_color_manual(values = color_palette[c(4,6)]) +
  facet_wrap(~type)

```

```{r, echo=FALSE}
ggplot(rhyme_rt_df, aes(talk_out_loud, rt, color=high_low_verbal)) +
  geom_sina(data= rhyme_rt_df_individual, aes(talk_out_loud, rt), alpha=0.3)+
  geom_errorbar(aes(ymin=rt-ci, ymax=rt+ci), width=.1, position= pd) +  
  stat_summary(fun = mean, geom = 'point', aes(group = high_low_verbal), position= pd) +
  stat_summary(fun = mean, geom = 'line', aes(group = high_low_verbal), size = 1, position= pd)+
  theme_bw() +
  labs(y ='RT', title = 'Rhyming x talking out loud')+
  scale_color_manual(values = color_palette[c(4,6)]) +
  facet_wrap(~type)
```

For both reaction time and accuracy, saying the words out loud diminished the difference between the two groups. This suggests that this was the strategy that high-verbal participants used in their heads - indeed, this was the most common strategy provided by the participants (from both groups) who chose to answer the free answer about strategy. There were no other notable strategies from the free answers.

## Verbal working memory

```{r,include=FALSE}
phon_sim_trials <- read.csv('phon_sim_trials_221028.csv',row.names = 1)
# include order as a variable for random effect
phon_sim_trials$presentation_order <- paste(phon_sim_trials$word_1,phon_sim_trials$word_2,phon_sim_trials$word_3,
                                            phon_sim_trials$word_4, phon_sim_trials$word_5, sep='_')
```

Participants were tested on recall of three sets of five words. One set contained words that were phonologically similar but not orthographically similar (bought, sort, taut, caught, and wart), one set contained words that were orthographically similar but not phonologically similar (rough, cough, through, dough, bough), and one set was a control set (plea, friend, sleigh, row, board).

#### Descriptive statistics by group: Verbal working memory

```{r, include=FALSE}
phonsim_desc_df_score <- phon_sim_trials %>%
  summarySEwithin2(measurevar = 'score', betweenvars = c('high_low_verbal'),
                  withinvars = 'original_word_set', idvar='worker_id', na.rm=T)
phonsim_desc_df_any_pos <- phon_sim_trials %>%
  summarySEwithin2(measurevar = 'score_any_position', betweenvars = c('high_low_verbal'),
                  withinvars = 'original_word_set', idvar='worker_id', na.rm=T)
phonsim_desc_df <- cbind(phonsim_desc_df_score, phonsim_desc_df_any_pos)
colnames(phonsim_desc_df) <- make.names(colnames(phonsim_desc_df), unique = T)
phonsim_desc_df <- phonsim_desc_df %>% select(high_low_verbal, original_word_set,
                                              score, ci, score_any_position, ci.1)
colnames(phonsim_desc_df)[4] <- 'ci_score'
colnames(phonsim_desc_df)[6] <- 'ci_score_any_position'

phonsim_score_df_individual_score <- phon_sim_trials %>%
  summarySEwithin2(measurevar = 'score', betweenvars = c('high_low_verbal','worker_id'),
                  withinvars = 'original_word_set', idvar='worker_id', na.rm=T)
phonsim_score_df_individual_score_any <- phon_sim_trials %>%
  summarySEwithin2(measurevar = 'score_any_position', betweenvars = c('high_low_verbal','worker_id'),
                  withinvars = 'original_word_set', idvar='worker_id', na.rm=T)
phonsim_desc_df_individual <- cbind(phonsim_score_df_individual_score, phonsim_score_df_individual_score_any)
colnames(phonsim_desc_df_individual) <- make.names(colnames(phonsim_desc_df_individual), unique = T)
phonsim_desc_df_individual <- phonsim_desc_df_individual %>% select(high_low_verbal, original_word_set,  score, ci, score_any_position, ci.1, worker_id)
colnames(phonsim_desc_df_individual)[4] <- 'ci_score'
colnames(phonsim_desc_df_individual)[6] <- 'ci_score_any_position'

phonsim_desc_df_individual <- pivot_longer(phonsim_desc_df_individual, cols = c('score', 'score_any_position'), names_to='score_type')
```

High verbal participants generally remembered more words correctly both when the correct position was required and when the words could be in any position (see table and figure below).

```{r, echo=FALSE}
phonsim_desc_df %>%
  kable(digits=2) %>%
  kable_styling(bootstrap_options = "striped")
```
```{r, echo=FALSE}
# word and position correct
ggplot(phonsim_desc_df_score, aes(original_word_set, score, color=high_low_verbal)) +
  geom_sina(data= phonsim_score_df_individual_score, aes(original_word_set, score), alpha=0.3)+
  geom_errorbar(aes(ymin=score-ci, ymax=score+ci), width=.1, position= pd) +  
  stat_summary(fun = mean, geom = 'point', aes(group = high_low_verbal), position= pd) +
  stat_summary(fun = mean, geom = 'line', aes(group = high_low_verbal), size = 1, position= pd)+
  theme_bw() +
  labs(y ='Accuracy (out of 5)', title = 'Verbal working memory')+
  scale_color_manual(values = color_palette[c(4,6)])

```

```{r, echo=FALSE}
# correct word regardless of position
ggplot(phonsim_desc_df_any_pos, aes(original_word_set, score_any_position, color=high_low_verbal)) +
  geom_sina(data= phonsim_score_df_individual_score_any, aes(original_word_set, score_any_position), alpha=0.3)+
  geom_errorbar(aes(ymin=score_any_position-ci, ymax=score_any_position+ci), width=.1, position= pd) +  
  stat_summary(fun = mean, geom = 'point', aes(group = high_low_verbal), position= pd) +
  stat_summary(fun = mean, geom = 'line', aes(group = high_low_verbal), size = 1, position= pd)+
  theme_bw() +
  labs(y ='Accuracy (out of 5 in any position)', title = 'Verbal working memory')+
  scale_color_manual(values = color_palette[c(4,6)])
```

#### Statistical models: Verbal working memory

```{r, include=FALSE, cache=TRUE}
# some models
score_by_verbal_m <- lmer(score ~ original_word_set * VerbalScored + 
                            (1|worker_id) + (1|presentation_order), phon_sim_trials)
summary(score_by_verbal_m)
# phon set more difficult than ctrl set, ortho set not more difficult
# no interaction with verbalscored
score_any_by_verbal_m <- lmer(score_any_position ~ original_word_set * VerbalScored + 
                                (1|worker_id) + (1|presentation_order), phon_sim_trials)
summary(score_any_by_verbal_m)

```

We conducted two linear mixed models of original word set (phonologically similar, orthographically similar, and control set) and verbal score predicting either memory performance with both correct word and correct position or memory performance with correct word regardless of position. Both models included random intercepts for each participant and for each presentation order of the stimuli.

For memory performance requiring both accurate word and position, the set with phonologically similar words was more difficult than the control set ($\beta$ = `r round(summary(score_by_verbal_m)$coefficients[3,1],2)`, SE = `r round(summary(score_by_verbal_m)$coefficients[3,2],2)`, t = `r round(summary(score_by_verbal_m)$coefficients[3,4], 2)`, p =  `r round(summary(score_by_verbal_m)$coefficients[3,5], 3)`) but the orthographically similar set was not ($\beta$ = `r round(summary(score_by_verbal_m)$coefficients[2,1],2)`, SE = `r round(summary(score_by_verbal_m)$coefficients[2,2],2)`, t = `r round(summary(score_by_verbal_m)$coefficients[2,4], 2)`, p =  `r round(summary(score_by_verbal_m)$coefficients[2,5], 3)`). A higher verbal score was associated with increased memory performance ($\beta$ = `r round(summary(score_by_verbal_m)$coefficients[4,1],2)`, SE = `r round(summary(score_by_verbal_m)$coefficients[4,2],2)`, t = `r round(summary(score_by_verbal_m)$coefficients[4,4], 2)`, p =  `r round(summary(score_by_verbal_m)$coefficients[4,5], 3)`). There was a marginally significant interaction effect ($\beta$ = `r round(summary(score_by_verbal_m)$coefficients[5,1],2)`, SE = `r round(summary(score_by_verbal_m)$coefficients[5,2],2)`, t = `r round(summary(score_by_verbal_m)$coefficients[5,4], 2)`, p =  `r round(summary(score_by_verbal_m)$coefficients[5,5], 3)`) which diminished the positive effect of higher verbal score on the orthographically similar set.

The same pattern was found when the correct word in any position counted as correct: The set with phonologically similar words was more difficult than the control set ($\beta$ = `r round(summary(score_any_by_verbal_m)$coefficients[3,1],2)`, SE = `r round(summary(score_any_by_verbal_m)$coefficients[3,2],2)`, t = `r round(summary(score_any_by_verbal_m)$coefficients[3,4], 2)`, p =  `r round(summary(score_any_by_verbal_m)$coefficients[3,5], 3)`) but the orthographically similar set was not ($\beta$ = `r round(summary(score_any_by_verbal_m)$coefficients[2,1],2)`, SE = `r round(summary(score_any_by_verbal_m)$coefficients[2,2],2)`, t = `r round(summary(score_any_by_verbal_m)$coefficients[2,4], 2)`, p =  `r round(summary(score_any_by_verbal_m)$coefficients[2,5], 3)`). A higher verbal score was associated with increased memory performance ($\beta$ = `r round(summary(score_any_by_verbal_m)$coefficients[4,1],2)`, SE = `r round(summary(score_any_by_verbal_m)$coefficients[4,2],2)`, t = `r round(summary(score_any_by_verbal_m)$coefficients[4,4], 2)`, p =  `r round(summary(score_any_by_verbal_m)$coefficients[4,5], 3)`). There was a significant interaction effect ($\beta$ = `r round(summary(score_any_by_verbal_m)$coefficients[5,1],2)`, SE = `r round(summary(score_any_by_verbal_m)$coefficients[5,2],2)`, t = `r round(summary(score_any_by_verbal_m)$coefficients[5,4], 2)`, p =  `r round(summary(score_any_by_verbal_m)$coefficients[5,5], 2)`) which diminished the positive effect of higher verbal score on the orthographically similar set.

#### Strategies: Verbal working memory

```{r, include=FALSE}
talk_out_loud_PS <- phon_sim_trials %>%
  group_by(worker_id, talk_out_loud, high_low_verbal) %>%
  tally()
# participant
tol_phon_sim <- talk_out_loud_PS %>%
  group_by(high_low_verbal,talk_out_loud) 
table(tol_phon_sim$high_low_verbal, tol_phon_sim$talk_out_loud)
chisq.test(tol_phon_sim$high_low_verbal, tol_phon_sim$talk_out_loud) #no difference
```
As with the rhyming experiment, we were again interested in whether participants said the words out loud to help them remember them. We asked about this at the end of the experiment. A chi-squared test showed that there was no significant difference between how many high-verbal participants (10 out of 47) and how many low-verbal participants (13 out of 46) reported that they had said the words out loud ($\chi^2$(1) = `r round(chisq.test(tol_phon_sim$high_low_verbal, tol_phon_sim$talk_out_loud)$statistic, 2)`, p = `r round(chisq.test(tol_phon_sim$high_low_verbal, tol_phon_sim$talk_out_loud)$p.value,3)`). Nevertheless, the effect of doing so was interestingly different for the two groups as can be seen in the figure below.

```{r, echo=F}
# does it matter what strategy they used?
phonsim_desc_df_score_TOL <- phon_sim_trials %>%
  summarySEwithin2(measurevar = 'score', betweenvars = c('high_low_verbal', 'talk_out_loud'),
                  withinvars = 'original_word_set', idvar='worker_id', na.rm=T)
phonsim_desc_df_any_pos_TOL <- phon_sim_trials %>%
  summarySEwithin2(measurevar = 'score_any_position', betweenvars = c('high_low_verbal', 'talk_out_loud'),
                  withinvars = 'original_word_set', idvar='worker_id', na.rm=T)
phonsim_desc_df_TOL <- cbind(phonsim_desc_df_score_TOL, phonsim_desc_df_any_pos_TOL)
colnames(phonsim_desc_df_TOL) <- make.names(colnames(phonsim_desc_df_TOL), unique = T)
phonsim_desc_df_TOL <- phonsim_desc_df_TOL %>% select(high_low_verbal, original_word_set,
                                              score, ci, score_any_position, ci.1,talk_out_loud)
colnames(phonsim_desc_df_TOL)[4] <- 'ci_score'
colnames(phonsim_desc_df_TOL)[6] <- 'ci_score_any_position'

phonsim_score_df_individual_score <- phon_sim_trials %>%
  summarySEwithin2(measurevar = 'score', betweenvars = c('high_low_verbal','worker_id', 'talk_out_loud'),
                  withinvars = 'original_word_set', idvar='worker_id', na.rm=T)
phonsim_score_df_individual_score_any <- phon_sim_trials %>%
  summarySEwithin2(measurevar = 'score_any_position', betweenvars = c('high_low_verbal','worker_id', 'talk_out_loud'),
                  withinvars = 'original_word_set', idvar='worker_id', na.rm=T)
phonsim_desc_df_individual <- cbind(phonsim_score_df_individual_score, phonsim_score_df_individual_score_any)
colnames(phonsim_desc_df_individual) <- make.names(colnames(phonsim_desc_df_individual), unique = T)
phonsim_desc_df_individual <- phonsim_desc_df_individual %>% select(high_low_verbal, original_word_set,  score, ci, score_any_position, ci.1, worker_id, talk_out_loud)
colnames(phonsim_desc_df_individual)[4] <- 'ci_score'
colnames(phonsim_desc_df_individual)[6] <- 'ci_score_any_position'

ggplot(phonsim_desc_df_TOL, aes(talk_out_loud, score, color=high_low_verbal)) +
  geom_sina(data= phonsim_score_df_individual_score, aes(talk_out_loud, score), alpha=0.3)+
  geom_errorbar(aes(ymin=score-ci_score, ymax=score+ci_score), width=.1, position= pd) +  
  stat_summary(fun = mean, geom = 'point', aes(group = high_low_verbal), position= pd) +
  stat_summary(fun = mean, geom = 'line', aes(group = high_low_verbal), size = 1, position= pd)+
  theme_bw() +
  labs(y ='Accuracy out of 5', title = 'Verbal working memory x talking out loud')+
  scale_color_manual(values = color_palette[c(4,6)])+
  facet_wrap(~original_word_set)
```

The difference between the two groups' memory performance disappears when they report that they said the words out loud to help them remember. Doing so helps low-verbal participants but makes no difference for high-verbal participants. Participants gave some interesting alternative strategies in response to the free answer question about strategies:

*High-verbal group*

* Remembering the order of the first letters once the words were familiar (e.g. c, b, t, r, d for 'cough', 'bough', 'through', 'rough', 'dough'). One participant reported this.
* Finding a cadence/melody and using this to repeat the words.
* Chunking.
* Hand and body gestures.
* Creating a story or a sentence with the words in order (both visual and verbal). This one was the most common strategy.

*Low-verbal group*

* Remembering the order of the first letters once the words were familiar (e.g. c, b, t, r, d for 'cough', 'bough', 'through', 'rough', 'dough'). This strategy was much more common for the low-verbal group than for the high-verbal group.
* Form a story or a narrative. This was a less common strategy than remembering the first letters.

## Task switching

We excluded trials over 10 seconds. We also recalculated the accuracy measure so that any trial in the three switch conditions where participants in fact switched between adding and subtracting counted as correct (as long as the arithmetic itself was also correct). We did this to prevent a failure to switch once resulting in the remaining trials counting as incorrect.

```{r, include=FALSE}
task_switch_trials <- read.csv('task_switch_trials_221028.csv',row.names = 1)
```

#### Descriptive statistics: Task switching

```{r, include=FALSE}
task_switch_desc_df_rt <- task_switch_trials %>%
  filter(switching_is_correct==1) %>%
  summarySEwithin2(measurevar = 'rt', betweenvars = 'high_low_verbal',
                  withinvars = 'condition', idvar='worker_id', na.rm=T)
task_switch_desc_df_acc <- task_switch_trials %>%
  summarySEwithin2(measurevar = 'switching_is_correct', betweenvars = 'high_low_verbal', withinvars = 'condition', idvar='worker_id', na.rm=T)
task_switch_desc_df <- cbind(task_switch_desc_df_rt,task_switch_desc_df_acc)
colnames(task_switch_desc_df) <- make.names(colnames(task_switch_desc_df),unique = T)
colnames(task_switch_desc_df)[8] <- 'ci_rt'
colnames(task_switch_desc_df)[16] <- 'ci_accuracy'
task_switch_desc_df <- task_switch_desc_df %>% select(high_low_verbal, condition, rt,ci_rt, switching_is_correct,ci_accuracy) 

task_switch_rt_df <- task_switch_trials %>%
  filter(correct == 1) %>%
  summarySEwithin2(measurevar = 'rt', betweenvars = 'high_low_verbal',
                  withinvars = 'condition',idvar = 'worker_id',na.rm=T)
task_switch_rt_df_individual <- task_switch_trials %>%
  filter(correct == 1) %>%
  summarySEwithin2(measurevar = 'rt', betweenvars = c('high_low_verbal', 'worker_id', 'talk_out_loud'),
                  withinvars = 'condition',idvar = 'worker_id', na.rm = T)
task_switch_correct_df <- task_switch_trials %>%
  summarySEwithin2(measurevar = 'switching_is_correct', betweenvars = c('high_low_verbal', 'talk_out_loud'), withinvars = 'condition',idvar = 'worker_id')
task_switch_correct_df_individual <- task_switch_trials %>%
  summarySEwithin2(measurevar = 'switching_is_correct', betweenvars = c('high_low_verbal', 'worker_id', 'talk_out_loud'),  withinvars = 'condition',idvar = 'worker_id')
```
As can be seen from the table and the figure below, accuracy was generally quite high in all conditions.

```{r, echo=FALSE}
task_switch_desc_df <- task_switch_desc_df %>%
  dplyr::mutate(condition = fct_relevel(condition, 
            "addition", "subtraction", "symbolcue", 
            "colorcue", "uncued")) 
task_switch_desc_df %>%
  dplyr::mutate(switching_is_correct = switching_is_correct*100) %>%
  kable(digits=2) %>%
  kable_styling(bootstrap_options = "striped")
```
```{r, echo=FALSE}
task_switch_rt_df$condition <- as.factor(task_switch_rt_df$condition)
task_switch_desc_df_rt <- task_switch_desc_df_rt %>%
  dplyr::mutate(condition = fct_relevel(condition, 
            "addition", "subtraction", "symbolcue", 
            "colorcue", "uncued")) 
task_switch_rt_df_individual <- task_switch_rt_df_individual %>%
  dplyr::mutate(condition = fct_relevel(condition, 
            "addition", "subtraction", "symbolcue", 
            "colorcue", "uncued")) 
task_switch_desc_df_rt %>% 
  ggplot(aes(condition, rt, color=high_low_verbal)) +
  geom_sina(data= task_switch_rt_df_individual, aes(condition, rt), alpha=0.3)+
  geom_errorbar(aes(ymin=rt-ci, ymax=rt+ci), width=.1, position= pd) +  
  stat_summary(fun = mean, geom = 'point', aes(group = high_low_verbal), position= pd) +
  stat_summary(fun = mean, geom = 'line', aes(group = high_low_verbal), size = 1, position= pd)+
  theme_bw() +
  labs(y ='RT per problem', title = 'Reaction time for task switching')+
  scale_color_manual(values = color_palette[c(4,6)])
```
```{r, echo=FALSE}
task_switch_desc_df_acc$condition <- as.factor(task_switch_desc_df_acc$condition)
task_switch_desc_df_acc <- task_switch_desc_df_acc %>%
  dplyr::mutate(condition = fct_relevel(condition, 
            "addition", "subtraction", "symbolcue", 
            "colorcue", "uncued")) 
task_switch_correct_df_individual <- task_switch_correct_df_individual %>%
  dplyr::mutate(condition = fct_relevel(condition, 
            "addition", "subtraction", "symbolcue", 
            "colorcue", "uncued")) 
task_switch_desc_df_acc %>% 
  ggplot(aes(condition, switching_is_correct, color=high_low_verbal)) +
  geom_sina(data= task_switch_correct_df_individual, aes(condition, switching_is_correct), alpha=0.3)+
  geom_errorbar(aes(ymin=switching_is_correct-ci, ymax=switching_is_correct+ci), width=.1, position= pd) +  
  stat_summary(fun = mean, geom = 'point', aes(group = high_low_verbal), position= pd) +
  stat_summary(fun = mean, geom = 'line', aes(group = high_low_verbal), size = 1, position= pd)+
  theme_bw() +
  labs(y ='Accuracy (corrected)', title = 'Accuracy for task switching')+
  scale_color_manual(values = color_palette[c(4,6)])
```

#### Statistical models: Task switching

```{r, include=FALSE}
switching_condition_symun_m <- glmer(switching_is_correct ~ condition*VerbalScored + (1|worker_id), 
                                     subset(task_switch_trials,condition %in% c('symbolcue','uncued')), family='binomial')
summary(switching_condition_symun_m) # uncued worse than symbolcued

switching_condition_symcol_m <- glmer(switching_is_correct ~ condition*VerbalScored + (1|worker_id), 
                                      subset(task_switch_trials,condition %in% c('colorcue','symbolcue')), family='binomial')
summary(switching_condition_symcol_m) # colorcue marginally worse than symbolcued


switching_rt_symun_m <- lmer(log(rt) ~ condition*VerbalScored + (1|worker_id), 
                              subset(task_switch_trials,condition %in% c('symbolcue','uncued') &
                                       switching_is_correct==1))
summary(switching_rt_symun_m) # no effects

switching_rt_symcol_m <- lmer(log(rt) ~ condition*VerbalScored + (1|worker_id), 
                               subset(task_switch_trials,condition %in% c('symbolcue','colorcue') &
                                        switching_is_correct==1))
summary(switching_rt_symcol_m) # symbolcue faster than colorcue
```
To simplify the comparisons, we only compared the symbol cue and the uncued conditions and the color cue and symbol cue conditions. In all models, participants were modeled as random intercepts. Linear mixed models of condition and verbal score predicting accuracy indicated no effect of verbal score (symbol cued versus uncued: $\beta$ = `r round(summary(switching_condition_symun_m)$coefficients[3,1],2)`, SE = `r round(summary(switching_condition_symun_m)$coefficients[3,2],2)`, z = `r round(summary(switching_condition_symun_m)$coefficients[3,3],2)`, p =  `r round(summary(switching_condition_symun_m)$coefficients[3,4],3)`; color cued versus symbol cued: $\beta$ = `r round(summary(switching_condition_symcol_m)$coefficients[3,1],2)`, SE = `r round(summary(switching_condition_symcol_m)$coefficients[3,2],2)`, z = `r round(summary(switching_condition_symcol_m)$coefficients[3,3],2)`, p =  `r round(summary(switching_condition_symcol_m)$coefficients[3,4],3)`). There were also no interaction effects (both p > `r round(summary(switching_condition_symun_m)$coefficients[4,4],3)`), but uncued trials were less likely to be accurate than symbol cued trials ($\beta$ = `r round(summary(switching_condition_symun_m)$coefficients[2,1],2)`, SE = `r round(summary(switching_condition_symun_m)$coefficients[2,2],2)`, z = `r round(summary(switching_condition_symun_m)$coefficients[2,3],2)`, p =  `r round(summary(switching_condition_symun_m)$coefficients[2,4],3)`).

As for log-transformed reaction time, there were also no effect of verbal score and no interaction effects (all p > `r round(summary(switching_rt_symun_m)$coefficients[4,5],3)`). However, symbol cued trials were marginally faster than color cued trials ($\beta$ = `r round(summary(switching_rt_symcol_m)$coefficients[2,1],2)`, SE = `r round(summary(switching_rt_symcol_m)$coefficients[2,2],2)`, t = `r round(summary(switching_rt_symcol_m)$coefficients[2,4],2)`, p =  `r round(summary(switching_rt_symcol_m)$coefficients[2,5],3)`).

#### Strategies: Task switching

```{r, include=FALSE}
talk_out_loud_TS <- task_switch_trials %>%
  group_by(worker_id, talk_out_loud, high_low_verbal) %>%
  tally()
tol_task_switch <- talk_out_loud_TS %>%
  group_by(high_low_verbal,talk_out_loud)
table(tol_task_switch$high_low_verbal, tol_task_switch$talk_out_loud)
chisq.test(tol_task_switch$high_low_verbal, tol_task_switch$talk_out_loud)
```

We once again examined differences associated with talking out loud, despite the fact that there were no general differences in performance between the two groups. A chi-squared test showed that there was no significant difference between how many high-verbal participants (20 out of 47) and how many low-verbal participants (13 out of 46) reported that they had talked to themselves out loud during the task ($\chi^2$(1) = `r round(chisq.test(tol_task_switch$high_low_verbal, tol_task_switch$talk_out_loud)$statistic, 2)`, p = `r round(chisq.test(tol_task_switch$high_low_verbal, tol_task_switch$talk_out_loud)$p.value,3)`). There were not any obvious differences between the effects that talking out loud had on these two groups (see accuracy and reaction time plots below).

```{r, echo=FALSE}
task_switch_rt_df %>%
ggplot(aes(condition, rt, color=high_low_verbal)) +
  geom_sina(data= task_switch_rt_df_individual, aes(condition, rt), alpha=0.3)+
  geom_errorbar(aes(ymin=rt-ci, ymax=rt+ci), width=.1, position= pd) +  
  stat_summary(fun = mean, geom = 'point', aes(group = high_low_verbal), position= pd) +
  stat_summary(fun = mean, geom = 'line', aes(group = high_low_verbal), size = 1, position= pd)+
  theme_bw() +
  labs(y ='RT', title = 'Task switching x talking out loud')+
  scale_color_manual(values = color_palette[c(4,6)])+
  facet_wrap(~talk_out_loud)+
  theme(axis.text.x = element_text(angle = 90))
```
```{r, echo=FALSE}
task_switch_correct_df %>%
ggplot(aes(condition, switching_is_correct, color=high_low_verbal)) +
  geom_sina(data= task_switch_correct_df_individual, aes(condition, switching_is_correct), alpha=0.3)+
  geom_errorbar(aes(ymin=switching_is_correct-ci, ymax=switching_is_correct+ci), width=.1, position= pd) +  
  stat_summary(fun = mean, geom = 'point', aes(group = high_low_verbal), position= pd) +
  stat_summary(fun = mean, geom = 'line', aes(group = high_low_verbal), size = 1, position= pd)+
  theme_bw() +
  labs(y ='Accuracy (corrected)', title = 'Task switching x talking out loud')+
  scale_color_manual(values = color_palette[c(4,6)]) +
  facet_wrap(~talk_out_loud) +
  theme(axis.text.x = element_text(angle = 90))
```

In response to the free answer question in the task switching experiment, several of the high-verbal participants said that they had said the answers out loud to themselves but not the operation ('add', 'subtract'). One visualized a cartoon character wearing red and giving thumbs-up or wearing blue and giving thumbs-down, one used their own thumb to keep track, and one used their fingers to count. Participants from the low-verbal group did not report many specific strategies apart from a few saying the operation or result out loud - one reported that they had tapped their index finger to mean 'add' and their middle finger to mean 'subtract'.

## Intertask correlations

We were interested in how performance on the different tasks correlated with each other and whether these correlations were different for the two groups.

```{r, include=FALSE, message=FALSE}
phonsim_score <- phon_sim_trials %>%
  group_by(worker_id, original_word_set) %>%
  dplyr::summarise(phonsim_score = mean(score, na.rm=T), phon_sim_any_pos = mean(score_any_position, na.rm=T))
phonsim_score <- pivot_wider(phonsim_score, names_from = original_word_set, values_from = c(phonsim_score, phon_sim_any_pos))
rhyme_score <- rhyming_trials %>%
  group_by(worker_id, type) %>%
  dplyr::summarise(rhyming_acc = mean(correct, na.rm=T))
rhyming_rt <- rhyming_trials %>%
  filter(correct ==1) %>%
  group_by(worker_id, type) %>%
  dplyr::summarise(rhyming_rt = mean(rt, na.rm=T))
rhyme_score <- merge(rhyme_score, rhyming_rt, by = c('worker_id', 'type'))
rhyme_score <- pivot_wider(rhyme_score, names_from = type, values_from = c(rhyming_acc, rhyming_rt))
# only take rt on correct trials
same_different_score <- same_different_trials %>%
  filter(correct ==1) %>%
  group_by(worker_id, judgment_type, answer) %>%
  dplyr::summarise(samediff_rt = mean(rt, na.rm=T))
same_different_score <- pivot_wider(same_different_score, names_from = c(judgment_type, answer), values_from = samediff_rt)
task_switching_score <- task_switch_trials %>%
  group_by(worker_id,condition) %>%
  dplyr::summarise(task_switch_acc=mean(switching_is_correct, na.rm=T))
task_switching_rt <- task_switch_trials %>%
  filter(switching_is_correct ==1) %>%
  group_by(worker_id,condition) %>%
  dplyr::summarise(task_switch_rt=mean(rt, na.rm=T))
task_switching_score <- merge(task_switching_score, task_switching_rt, 
                              by = c('worker_id', 'condition'))
task_switching_score <- pivot_wider(task_switching_score, names_from = condition, values_from = c(task_switch_rt, task_switch_acc))

score_df_list <- list(phonsim_score,rhyme_score,
                      same_different_score,task_switching_score)
score_df <- score_df_list %>% reduce(full_join, by='worker_id')
# put verbal scores in
score_df <- merge(score_df, irq_scores, by ='worker_id', all.x = T)
score_df <- distinct(score_df)

```

#### Overall intertask correlations

Colored squares are significant at p < .01. Generally, different performance measures correlate within the same experiment. Interestingly, reaction times on rhyming are negatively correlated with verbal working memory score suggesting some working memory involvement in the rhyming task. Accuracy on uncued switch trials in the task switching experiment is also positively correlated with accuracy on verbal working memory and rhyming. 

```{r, echo=FALSE,message=FALSE}
library(Hmisc)
correlations <- rcorr(as.matrix(score_df[2:29]))
cors <- correlations$r
ps <- cor.mtest(as.matrix(score_df[2:29]))$p
corrplot(correlations$r, addgrid.col='black',method="color", p.mat=ps, sig.level = 0.01,insig='blank', tl.cex = 0.5)
```

#### Intertask correlations for the *high-verbal group*

Colored squares are significant at p < .01. For high-verbal participants, reaction times on rhyming, category judgments, and task switching are positively correlated, suggesting that these rely on similar mechanisms for this group.

```{r, echo=FALSE}
score_df_high_verbal <- subset(score_df, high_low_verbal =='high_verbal')
correlations_hv <- rcorr(as.matrix(score_df_high_verbal[2:29]))
cors_hv <- correlations_hv$r
ps_hv <- cor.mtest(as.matrix(score_df_high_verbal[2:29]))$p
corrplot(correlations_hv$r, method="color",addgrid.col='black',p.mat=ps_hv, sig.level = 0.01,insig='blank', tl.cex = 0.5)

```

#### Intertask correlations for the *low-verbal group*

Colored squares are significant at p < .01. For low-verbal participants, there is no such widespread correlation between rhyming, category judgments, and task switching reaction times. However, they show a positive correlation between accuracy on uncued switch trials and (position-indifferent) verbal working memory and rhyming accuracy.

```{r, echo=FALSE}
score_df_low_verbal <- subset(score_df, high_low_verbal =='low_verbal')
correlations_lv <- rcorr(as.matrix(score_df_low_verbal[2:29]))
cors_lv <- correlations_lv$r
ps_lv <- cor.mtest(as.matrix(score_df_low_verbal[2:29]))$p
corrplot(correlations_lv$r, method="color", addgrid.col='black',p.mat=ps_lv, sig.level = 0.01,insig='blank', tl.cex = 0.5)

```

#### Difference between task correlations in *low-verbal group* and *high-verbal group*

Colored squares are significant at p < .01.

```{r, echo=FALSE}

cors_subtracted <- correlations_lv$r - correlations_hv$r
corrplot(cors_subtracted, method="color", addgrid.col='black',p.mat=ps_lv,insig='blank', tl.cex = 0.5)

```

## Summary of behavioral findings

Participants who scored lower on verbal representations were slower at making category-based judgments in the same-different experiment. We expected high-verbal participants to show a more marked effect of being distracted by category membership on identity judgment trials (e.g. being slower at correctly responding 'different' to two pictures of different cats). We did not see this in the data.

In the rhyming experiment, high-verbal participants were significantly better than low-verbal participants. The differences in both accuracy and response time between the two groups was eliminated when participants reported naming the pictures out loud. 

In the phonological similarity experiment, the high-verbal group performed better at both position-specific recall and position-indifferent recall. Once again, the differences between the two groups was diminished if participants reported talking out loud to remember the words.

There were no notable differences between the two groups in the task switching experiment.

## Questionnaire measures

```{r, include=FALSE}
Q_anendophasia_numeric <- read.csv('alexinoia_questionnaire_numeric_wide.csv', row.names = 1)
Q_anendophasia_text <- read.csv('alexinoia_questionnaire_text_wide.csv', row.names = 1)

ptcpts_exp <- unique(rhyming_trials$worker_id)
ptcpts_q <- unique(Q_anendophasia_numeric$worker_id)
length(intersect(ptcpts_exp,ptcpts_q))

ptcpts_exp[which(!ptcpts_exp %in% ptcpts_q)] # A3KVKK1XLBTSN3 is missing from the questionnaire data? 

table(Q_anendophasia_numeric$high_low_verbal)
```

For some strange reason, we do not have questionnaire data from A3KVKK1XLBTSN3. We will retain their data from the four behavioral experiments and here report questionnaire data from 47 high-verbal and 45 low-verbal participants.

```{r, include=FALSE, cache = TRUE}
# get the variables of interest
# worker_id, express_inner_state, rehearse_convo, revise_convo etc
questionnaire_numeric_interest <- Q_anendophasia_numeric[,c(1,18,20,21,22,24,26,28,29,30,32,34:46, 120:122)]
questionnaire_text_interest <- Q_anendophasia_text[,c(1,18,20,21,22,24,26,28,29,30,32,34:46, 120:122)]

# plot all questions in preparation for One Big Plot


# REHEARSE QUESTION IN LECTURE
rehearse_question_labs <- c('I rehearse in my mind\nsome of what I am going to\nask before asking it', 'I rehearse in my mind\nthe exact phrasing of what\nI am going to ask', 'I think of a\nquestion I want to ask\nand just ask it', 'Other',"I'm never in a\nposition to ask questions in\nfront of an audience")
rehearse_question_p <- ggplot(questionnaire_text_interest, aes(rehearse_question,fill=high_low_verbal)) +
  geom_bar(position="dodge", aes(y = (..count..)/sum(..count..)), width = 0.5)+
  theme_bw()+
  labs(title='If you have to ask a question in front of an audience,\nwhich of these best describes what you typically do?', x='', y= '') +
    theme(plot.title = element_text(size = 12, face = "bold"), axis.title = element_text(size = 10),
        axis.text.x = element_text(size = 10, angle = 90, hjust = 1), legend.position = 'none')+
  scale_x_discrete(labels=rehearse_question_labs) +
  scale_fill_manual(values = color_palette[c(4,6)])

# REAL-LIFE CONVERSATION FOCUS
likert_order <- c('Never', 'Rarely', 'Sometimes', 'Often','Always')
conversation_focus_p <- ggplot(questionnaire_text_interest, aes(factor(conversation_focus,level=likert_order),fill=high_low_verbal)) +
  geom_bar(position="dodge", aes(y = (..count..)/sum(..count..)), width = 0.5)+
  theme_bw()+
  labs(title='How often do you experience trouble focusing on \na face-to-face conversation you are having because of a conflicting \nconversation happening in your mind at the same time?', x='', y= '') +
    theme(plot.title = element_text(size = 12, face = "bold"), axis.title = element_text(size = 10),
        axis.text.x = element_text(size = 10, angle = 90, hjust = 1), legend.position = 'none')+
  scale_fill_manual(values = color_palette[c(4,6)])

# EARWORMS
freq_order <- c('Never', 'A few times a year', 'A few times a month', 'A few times a week','Multiple times a day')
earworms_p <- ggplot(questionnaire_text_interest, aes(factor(earworms, level=freq_order), fill=high_low_verbal)) +
  geom_bar(position="dodge", aes(y = (..count..)/sum(..count..)), width = 0.5)+
  theme_bw()+
  labs(title = 'How often do you have songs stuck in your head?', x='', y='')+
  theme(plot.title = element_text(size = 12, face = "bold"), axis.title = element_text(size = 10),
        axis.text.x = element_text(size = 10, angle = 90, hjust = 1), legend.position = 'none')+
  scale_fill_manual(values = color_palette[c(4,6)])

# REMEMBER FRIEND CONVO
labels <- c("I can easily recall it.\nIf I wrote it down and\nmatched to a recording\nof the conversation,\nthere'd be an\nalmost perfect match",     
"I remember the topic\nand remember much of what\nwas said. If I matched\nit to a recording of the\nconversation, a lot\nwould match up.",
"I remember the topic,\nbut can't remember\nany of the specifics." ,                                                                   
"I remember the topic,\nbut remember only a few\nof the specific\nwords/sentences."  ,                                                  
"Other")
convo_memory_friend_p <- ggplot(questionnaire_text_interest, aes(convo_memory_friend, fill=high_low_verbal)) +
  geom_bar(position="dodge", aes(y = (..count..)/sum(..count..)), width = 0.5)+
  theme_bw()+
  labs(title = 'If you had to recall a short conversation \nabout a specific topic that you had yesterday with a friend, \nhow easily can you recall the exact words your friend said?', x='', y='')+
  theme(plot.title = element_text(size = 12, face = "bold"), axis.title = element_text(size = 10),
        axis.text.x = element_text(size = 10, angle = 90, hjust = 1), legend.position = 'none')+
  scale_x_discrete(labels=labels) +
  scale_fill_manual(values = color_palette[c(4,6)])

# REMEMBER SELF CONVO
convo_memory_self_p <- ggplot(questionnaire_text_interest, aes(convo_memory_self, fill=high_low_verbal)) +
  geom_bar(position="dodge", aes(y = (..count..)/sum(..count..)), width = 0.5)+
  theme_bw()+
  labs(title = 'If you had to recall a short conversation \nabout a specific topic that you had yesterday with a friend, \nhow easily can you recall the exact words you said?', x='', y='')+
  theme(plot.title = element_text(size = 12, face = "bold"), axis.title = element_text(size = 10),
        axis.text.x = element_text(size = 10, angle = 90, hjust = 1), legend.position = 'none')+
  scale_x_discrete(labels=labels) +
  scale_fill_manual(values = color_palette[c(4,6)])

# HEARING CONVERSATION
hearing_levels <- c("I can't hear it,\nbut I can still recall it.\nPlease briefly say something\nabout how you are recalling it.",
"I hear a condensed version\n(e.g. only some words).",
"I hear something \nbut I can't describe it.",
"It's just like I'm\nhearing the conversation again.")
hearing_conversation_p <- ggplot(questionnaire_text_interest, aes(hearing_conversation, fill=high_low_verbal)) +
  geom_bar(position="dodge", aes(y = (..count..)/sum(..count..)), width = 0.5)+
  theme_bw()+
  labs(title = 'When you recall a conversation,\ndo you hear the words in your mind?', x='', y='')+
  theme(plot.title = element_text(size = 12, face = "bold"), axis.title = element_text(size = 10),
        axis.text.x = element_text(size = 10, angle = 90, hjust = 1), legend.position = 'none')+
  scale_x_discrete(labels=hearing_levels) +
  scale_fill_manual(values = color_palette[c(4,6)])

# SING ALONG
sing_along_levels <- c("No - I can't imagine\nhow anyone could do this", "No - but I can imagine\nhow others can do it",
                       "Yes - somewhat","Yes - definitely")
sing_along_p <- ggplot(questionnaire_text_interest, aes(sing_along, fill=high_low_verbal)) +
  geom_bar(position="dodge", aes(y = (..count..)/sum(..count..)), width = 0.5)+
  theme_bw()+
  labs(title = 'Can you "sing along" to music without singing out loud?', x='', y='')+
  theme(plot.title = element_text(size = 12, face = "bold"), axis.title = element_text(size = 10),
        axis.text.x = element_text(size = 10, angle = 90, hjust = 1), legend.position = 'none')+
  scale_x_discrete(labels=sing_along_levels) +
  scale_fill_manual(values = color_palette[c(4,6)])

# SING ALONG THINKING
thinking_levels <- c("Exactly like regular thinking", "I can't sing along\nwithout singing out loud","Mostly different\nfrom regular thinking" , "Mostly similar\nto regular thinking","Neutral",  "Not at all")
sing_along_thinking_p <- ggplot(questionnaire_text_interest, aes(sing_along_thinking, fill=high_low_verbal)) +
  geom_bar(position="dodge", aes(y = (..count..)/sum(..count..)), width = 0.5)+
  theme_bw()+
  labs(title = 'If you can "sing along" to music without singing out loud,\nto what extent does this feel like regular thinking?', x='', y='')+
  theme(plot.title = element_text(size = 12, face = "bold"), axis.title = element_text(size = 10),
        axis.text.x = element_text(size = 10, angle = 90, hjust = 1), legend.position = 'none')+
  scale_x_discrete(labels=thinking_levels) +
  scale_fill_manual(values = color_palette[c(4,6)])

# EXPERIENCE OTHERS' VOICES
voice_qual_labs <- c('I hear what they\nsay in their voice.',
'I hear what they\nsay but in my own voice.',
"I hear the words but\nI can't tell whose voice it is.",
'I don’t “hear” anything,\nI imagine it by...')
other_voice_quality_p <- ggplot(questionnaire_text_interest, aes(other_voice_quality, fill=high_low_verbal)) +
  geom_bar(position="dodge", aes(y = (..count..)/sum(..count..)), width = 0.5)+
  theme_bw()+
  theme(plot.title = element_text(size = 12, face = "bold"), axis.title = element_text(size = 10),
        axis.text.x = element_text(size = 10, angle = 90, hjust = 1), legend.position = 'none')+  
  labs(title = 'If you imagine someone else speaking, how do you experience their voice?',x='', y='') +
  scale_x_discrete(labels=rev(voice_qual_labs)) +
  scale_fill_manual(values = color_palette[c(4,6)])

# THINKING IN IDEAS
ideas_labs <- c('"Thinking in ideas"', 'Conversation')
thinking_in_ideas_p <- ggplot(questionnaire_text_interest, aes(thinking_in_ideas,fill=high_low_verbal)) +
  geom_bar(position="dodge", aes(y = (..count..)/sum(..count..)), width = 0.5)+
  theme_bw()+
  labs(title ='Many people feel that a lot of their thinking, planning,\nand decision-making takes place in the form of a conversation with themselves.\nThey describe that when they think, they hear words in their mind.\nOther people don’t have this experience and instead say\nthat they "think in ideas". Is your experience\nmore like the first or the second?',
       x='', y='Proportion of total responses')+
  theme(plot.title = element_text(size = 12, face = "bold"), axis.title = element_text(size = 10),
        axis.text.x = element_text(size = 10, angle = 90, hjust = 1), legend.position = 'none')+
  scale_x_discrete(labels=ideas_labs) +
  scale_fill_manual(values = color_palette[c(4,6)])

# EXPRESS INNER STATES - it is generally difficult
agree_levs <- c("Strongly agree","Agree" , "Neither agree not disagree", "Disagree",  "Strongly disagree")
express_inner_states_p <- ggplot(questionnaire_text_interest, aes(factor(express_inner_states, levels=agree_levs), fill=high_low_verbal)) +
  geom_bar(position="dodge", aes(y = (..count..)/sum(..count..)), width = 0.5)+
  theme_bw()+
  theme(plot.title = element_text(size = 12, face = "bold"), axis.title = element_text(size = 10),
        axis.text.x = element_text(size = 10, angle = 90, hjust = 1), legend.position = 'none')+  
  labs(title="'It is generally difficult and takes effort\nto express in words how I think and feel.'",
       x='', y='')+
  scale_fill_manual(values = color_palette[c(4,6)])

# it sounds stressful to have an inner voice
stress_IS_p <- ggplot(questionnaire_text_interest, aes(stress_IS, fill=high_low_verbal)) +
  geom_bar(position="dodge", aes(y = (..count..)/sum(..count..)), width = 0.5)+
  theme_bw()+
  theme(plot.title = element_text(size = 12, face = "bold"), axis.title = element_text(size = 10),
        axis.text.x = element_text(size = 10, angle = 90, hjust = 1), legend.position = 'none')+
  labs(title='Do you think it is stressful and annoying to have an inner monologue?', x='', y='')+
  scale_fill_manual(values = color_palette[c(4,6)])
# no difference

# inner speech in books and movies
narrative_labs <- c("It might be like real life but\nmostly for the viewer's/reader's benefit",
"It's exactly like real life",
"It's just for the viewer/reader's benefit")
narrative_IS_p <-ggplot(questionnaire_text_interest, aes(narrative_IS, fill=high_low_verbal)) +
  geom_bar(position="dodge", aes(y = (..count..)/sum(..count..)), width = 0.5)+
  theme_bw()+
  theme(plot.title = element_text(size = 12, face = "bold"), axis.title = element_text(size = 10),
        axis.text.x = element_text(size = 10, angle = 90, hjust = 1), legend.position= 'none')+
  labs(title='In books and movies, we often see characters talking to themselves\nat length. How much do you think this reflects real life?',x='', y='')+
    scale_x_discrete(labels=narrative_labs) +
  scale_fill_manual(values = color_palette[c(4,6)])

# revise conva
likert_order <- c('Never', 'Rarely', 'Sometimes', 'Often','Very often')
revise_convo_p <- ggplot(questionnaire_text_interest, aes(factor(revise_convo, levels=likert_order), fill=high_low_verbal)) +
  geom_bar(position="dodge", aes(y = (..count..)/sum(..count..)), width = 0.5)+
  theme_bw()+
  theme(plot.title = element_text(size = 12, face = "bold"), axis.title = element_text(size = 10),
        axis.text.x = element_text(size = 10, angle = 90, hjust = 1), legend.position = 'none')+
  labs(title='Do you ever revise past conversations in your mind?', x='', y='')+
  scale_fill_manual(values = color_palette[c(4,6)])

# rehearse convo
rehearse_convo_p <- ggplot(questionnaire_text_interest, aes(factor(rehearse_convo, levels=likert_order), fill=high_low_verbal)) +
  geom_bar(position="dodge", aes(y = (..count..)/sum(..count..)), width = 0.5)+
  theme_bw()+
  theme(plot.title = element_text(size = 12, face = "bold"), axis.title = element_text(size = 10),
        axis.text.x = element_text(size = 10, angle = 90, hjust = 1), legend.position = 'none')+
  labs(title='Do you ever rehearse a conversation before you have it in real life\nwhere you simulate what you will say and how the other person will respond?',x='', y='')+
  scale_fill_manual(values = color_palette[c(4,6)])

# experience type while falling asleep
exp_type_levs <- c("An even mix of verbal, visual,\nsensory, and emotional",                                  
                   "My inner experience in that situation \ndoesn't have a specific \"format\"",                     
                   "Primarily about sensory awareness (what \nyou are hearing, smelling, and \nfeeling in the moment)",
                   "Primarily emotional",                                                                          
                   "Primarily verbal (you \"hear\" or\n \"speak\" words and sentences in your mind)",                
                   "Primarily visual (you \"see\" situations,\n objects, people etc. in your mind)")
exp_type_nodding_off_p <- ggplot(questionnaire_text_interest, aes(exp_type_nodding_off, fill=high_low_verbal)) +
  geom_bar(position="dodge", aes(y = (..count..)/sum(..count..)), width = 0.5)+
  theme_bw()+
  theme(plot.title = element_text(size = 12, face = "bold"), axis.title = element_text(size = 10),
        axis.text.x = element_text(size = 10, angle = 90, hjust = 1), legend.position = 'none')+
  scale_x_discrete(labels=exp_type_levs)+
  labs(title='What is your experience like when you are trying to fall asleep?',x='', y='')+
  scale_fill_manual(values = color_palette[c(4,6)])

level_order <- c('Strongly disagree', 'Disagree', 'Neither agree nor disagree', 'Agree','Strongly agree')
# impulsiveness 1: i don't know why i do some of the things that i do
impulsiveness_1_p <- ggplot(questionnaire_text_interest, aes(factor(impulsiveness_1, level = level_order),fill=high_low_verbal)) +
  geom_bar(position="dodge", aes(y = (..count..)/sum(..count..)), width = 0.5)+
  theme_bw()+
  theme(plot.title = element_text(size = 12, face = "bold"), axis.title = element_text(size = 10),
        axis.text.x = element_text(size = 10, angle = 90, hjust = 1), legend.position = 'none')+
  labs(title="'I do not know why I do some of the things that I do'",x='', y='')+
  scale_fill_manual(values = color_palette[c(4,6)])

# impulsiveness 2: i am a firm believer in thinking things through
impulsiveness_2_p <- ggplot(questionnaire_text_interest, aes(factor(impulsiveness_2, level=level_order),fill=high_low_verbal)) +
  geom_bar(position="dodge", aes(y = (..count..)/sum(..count..)), width = 0.5)+
  theme_bw()+
  theme(plot.title = element_text(size = 12, face = "bold"), axis.title = element_text(size = 10),
        axis.text.x = element_text(size = 10, angle = 90, hjust = 1), legend.position = 'none')+
  labs(title="'I believe in thinking things through'",x='', y='')+
  scale_fill_manual(values = color_palette[c(4,6)])

# impulsiveness 1: i like to act on a whim
impulsiveness_3_p <- ggplot(questionnaire_text_interest, aes(factor(impulsiveness_3, level=level_order),fill=high_low_verbal)) +
  geom_bar(position="dodge", aes(y = (..count..)/sum(..count..)), width = 0.5)+
  theme_bw()+
  theme(plot.title = element_text(size = 12, face = "bold"), axis.title = element_text(size = 10),
        axis.text.x = element_text(size = 10, angle = 90, hjust = 1), legend.position = 'none')+
  labs(title="'I like to act on a whim'",x='', y='')+
  scale_fill_manual(values = color_palette[c(4,6)])

```

```{r, include=FALSE}
# put all the plots together
combined_q_plot <- plot_grid(rehearse_question_p, conversation_focus_p, earworms_p, convo_memory_friend_p,convo_memory_self_p, hearing_conversation_p, sing_along_p, sing_along_thinking_p, other_voice_quality_p, thinking_in_ideas_p, express_inner_states_p, stress_IS_p, narrative_IS_p, revise_convo_p, rehearse_convo_p, exp_type_nodding_off_p, impulsiveness_1_p, impulsiveness_2_p, impulsiveness_3_p,
                             labels = c('A', 'B', 'C', 'D','E', 'F', 'G','H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S'),
                             label_size = 12,
                             nrow=5, ncol=4)
#ggsave('combined_q_plot.png', combined_q_plot, width = 30, height = 40)
```

Here is a plot of all our custom questions. Dark blue represents the high-verbal group and dark yellow represents the low-verbal group.

![Questionnaire answers.](combined_q_plot.png).


